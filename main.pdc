---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez"
    affiliation: "UC Santa Cruz"
    email: "ivo@cs.ucsc.edu"
  - name: "Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "carlosm@ucsc.edu"
  - name: "Adam Moody"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "moody11@llnl.gov"
  - name: "Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "kathryn@llnl.gov"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "gflofst@sandia.gov"
  - name: "Remzi Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "remzi@cs.wisc.edu"
abstract: |
  Evaluating experimental results in the field of Computer Systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we analyze salient features of container technology 
  that, if leveraged correctly, can help reducing the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of storage systems to illustrate the extensions that we 
  envision, mainly in terms of container management infrastructure. We 
  also discuss the benefits and limitations of using containers as a 
  way of reproducing research in other areas of experimental systems 
  research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
documentclass: acm-sig-alternate
substitute-hyperref: true
disable-copyright: true
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous results. Registering information about an 
experiment (referred to as _provenance_) such as annotations, diagrams 
and lists, allows scientists to interpret and understand results. By 
examining the steps that led to a result, scientists can gain insights 
into the chain of reasoning used in its production, verify that the 
experiment was performed according to acceptable procedures, identify 
the experiment's inputs, and, in some cases, regenerate the original 
result [@freire_provenance_2008]. Additionally, reproducibility plays 
a major roll in education since the amount of information that a 
student has to digest increases as the pace of scientific discovery 
accelerates. By having repeatable experiments, a student can learn by 
looking at provenance information, re-evaluate the questions that the 
original experiment answered and thus "stand in the shoulder of 
giants".

In applied Computer Science an experiment is carried out in its 
entirety by a computer [@freire_computational_2012 ; 
@donoho_reproducible_2009]. Although the scientist is interactively 
defining the sequence of steps that get executed, ultimately binary 
programs run on a machine. Repeating a result doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`). With our current 
computational abilities to collect and manage data we should be able 
to record and reconstruct environments for particular results 
perfectly. However, this is not the case, mainly due to three factors:

  1. Association between experiment and its codebase.
  2. Evolving software.
  3. Changes in hardware.

Version control systems (VCS) are sometimes used to tackle 1 and 2. By 
having a particular version ID for the software used for an article's 
experimental results, reviewers and readers can have access to the 
same codebase [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_measuring_2014] 
since the code might not compile and, even if compilable, the results 
might differ, in which case the differences have to be analyzed in 
order to corroborate the validity of the original experiment.

Reproducing experimental results when the underlying hardware 
environment changes (3) is challenging mainly due to the inability of 
predicting the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads associated in terms of performance (the 
hypervisor's "tax") and management (creating, storing and transferring 
them) can be high and, in some fields of Computer Science such as 
Computer Systems research, cannot be accounted easily 
[@clark_xen_2004].

Container technology [@soltesz_container-based_2007] is currently 
employed as a way of reducing the complexity of software deployment in 
cloud computing infrastructure and has taken the role that package 
management tools had in the past, where they were used to control 
upgrades and keep track of change in the dependencies of an 
application [@dicosmo_package_2008]. In this work, we make the case 
for containers as a way of tackling some of the reproducibility 
problems in Computer Systems research. In order to reduce the problem 
space, we focus to local and distributed storage systems in various 
forms (e.g., local file systems, distributed file systems, key-value 
stores, and related data-storage engines) since this is one of the 
most important areas underlying cloud computing and big-data 
processing, as well as our area of expertise.

We first describe the salient features of container technology that 
are relevant in the evaluation of experiments. We then describe what 
in our view is missing in order to make containers a useful 
reproducibility tool for storage systems research. We then present a 
use case that illustrates the container management extensions that we 
envision that would allow to record metadata about the execution of 
experiments. We then describe how this metadata can be used to 
evaluate an experiment and, at the same time, create a catalog that 
researches can use when looking for particular types of experiments. 
We then discuss the benefits and limitations of using containers as a 
way of reproducing research in other areas of experimental systems 
research. We conclude and delineate future work.

# Experimental Evaluation in Computer Systems Research

An experiment in systems research is usually composed by a triplet of 
\(1\) workload, (2) a specific system where the workload runs and 
\(3\) associated results to a particular execution. Respective to this 
order is the complexity associated to the evaluation of an experiment: 
obtaining the exact same results is more difficult than getting access 
to the original workload.

**TODO: extend the above in order to define reproducibility levels**

\ 

**TODO: also, talk about how container features help with each level 
of reproducibility: metrics, deployment, automation**

# Containers For Reproducible Research

For experiments suitable to be containerized, we define a mapping 
methodology that allows to evaluate experimental results on hardware 
distinct than the original:

  1. Obtain the profile of the original hardware
  2. Obtain the configuration of each container (e.g. User Beacons or 
     cgroup configuration)
  3. Obtain the profile of the new hardware
  4. Generate a configuration for the new hardware based on 2-4


# Use Case: Reproducing Results on Different Storage Devices

We compare:

  * An old hard drive
  * The equivalent (modern) model of an old hard drive
  * An SSD

We will show that:

  * Direct I/O is reproducible
  * Asynchronous I/O is not
  * Therefore we need to extend containers to handle async requests on 
    a per-container basis (not on a global)

This case will also illustrate how having the HW profile and base 
configuration of containers can help in:

  * mapping to new hardware
  * mapping to old-but-aged hardware

# Discussion

We discuss benefits and limitations of containers, as well as general 
guidelines.

## Cataloging Experiments

We can characterize experiments by creating categories of container 
metric profiles, e.g.:

  * In-memory only
  * Storage intensive
  * Network intensive
  * CPU intensive
  * xx% of caching effects

We need to profile an experiment, store the profile along with the 
image of the container, categorize the profile and index it.

## Can All Systems Research Be Containerized?

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * More than one container needs to be co-located in the same 
    physical host
  * Kernel version can't be frozen

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects.

## Other Lessons Learned So Far

  1. Version control everything and use git subtrees or submodules to 
     keep track of inter-dependencies between projects. For example, 
     if a repository contains the definition of a container, make it a 
     submodule of the main project.
  2. Refer to the specific version ID that a paper's results were 
     obtained from. Git's tagging feature can also be used to point to 
     the version that contains the codebase for an experiment (e.g. 
     "sosp15")
  3. Keep a container image for every runnable version of the 
     experiment codebase (e.g. use docker's automated build feature).
  4. Whenever possible, use CI technologies to ensure that changes to 
     the codebase don't disrupt the reproducibility of the experiment.
  5. Even without something like T-RECS, it is useful to obtain a 
     profile of the hardware used (eg. making use of tools such as 
     SoSReport[^sosreport]), as well as the cgroups configuration and 
     publish these as part of the experimental results (i.e. add it to 
     the commit that a paper's results are based on).

# Related Work

  * Recent efforts such as _The Recomputation Manifesto_ [^recomp] 
    leave runtime performance as a secondary problem. For most of the 
    work in Systems Research, runtime performance _is_ the subject of 
    research thus we need to look at it as a primary issue.

  * How we differ/extend the work in [@collberg_measuring_2014]? 
    **Answer**: we are interested not only in being able to rebuild 
    binaries but also evaluate the validity of the results.

  * Existing tools such as Reprozip [@ ] try to package an experiment 
    in a container without having to implement it in one (i.e. 
    automates the creation of a container from an "uncontainerized" 
    environment). Our work is complementary in the sense that we look 
    at the conditions in which the experiment can be validated in 
    order tu ensure that the containerized version will (or won't) 
    produce valid results.

# Conclusion and Future Work

Take papers from other sub-disciplines of Systems Research:

  * Kernel
  * Network
  * Hardware

How can we repeat the effort presented in this paper for the above?

# References

[^sosreport]: <https://www.github.com/sosreport/sos>

[^recomp]: <http://www.recomputation.org/manifesto/>

[fio]: https://github.com/axboe/fio/

<!-- hanged biblio-->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
