---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez and Carlos Maltzahn"
    affiliation: "_University of California Santa Cruz_"
    email: "`{ivo,carlosm}@cs.ucsc.edu`"
  - name: "Adam Moody and Kathryn Mohror"
    affiliation: "_Lawrence Livermore National Laboratories_"
    email: "`{moody11,kathryn}@llnl.gov`"
  - name: "Jay Lofstead"
    affiliation: "_Sandia National Laboratories_"
    email: "`gflofst@sandia.gov`"
  - name: "Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau"
    affiliation: "_University of Wisconsin-Madison_"
    email: "`{remzi,dusseau}@cs.wisc.edu`"
abstract: |
  Evaluating experimental results in the field of computer systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we analyze salient features of container technology 
  that, if leveraged correctly, can help reduce the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of distributed storage systems to illustrate the 
  extensions that we envision, mainly in terms of container management 
  infrastructure. We also discuss the benefits and limitations of 
  using containers as a way of reproducing research in other areas of 
  experimental systems research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
substitute-hyperref: true
links-as-notes: true
documentclass: ieeetran
classoption: conference
fontfamily: times
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous experiments. Registering information about an 
experiment allows scientists to interpret and understand results, as 
well as verify that the experiment was performed according to 
acceptable procedures. Additionally, reproducibility plays a major 
role in education since the amount of information that a student has 
to digest increases as the pace of scientific discovery accelerates. 
By having the ability to repeat experiments, a student can learn by 
looking at provenance information, re-evaluate the questions that the 
original experiment answered and thus "stand on the shoulder of 
giants".

In applied computer science an experiment is carried out in its 
entirety on a computer. Repeating an experiment doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`); however, this is 
not the case. Today's computational environments are complex and 
accounting for all possible effects of changes within and across 
systems is a challenging task [@freire_computational_2012 ; 
@donoho_reproducible_2009].

Version-control systems (VCS) are sometimes used to address some of 
these problems. By having a particular version ID for the software 
used for an article's experimental results, reviewers and readers can 
have access to the same code base [@brown_how_2014]. However, 
availability of the source code does not guarantee reproducibility 
[@collberg_measuring_2014] since the code might not compile and, even 
if compilable, the results might differ. In that case, the differences 
have to be analyzed in order to corroborate the validity of the 
original experiment.

Additionally, reproducing experimental results when the underlying 
hardware environment changes is challenging mainly due to the 
inability to predict the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads in terms of performance (the hypervisor 
"tax") and management (creating, storing and transferring) can be high 
and, in some fields of computer science such as systems research, 
cannot be accounted for easily [@clark_xen_2004].

OS-level virtualization [@soltesz_container-based_2007] is a server 
virtualization method where the kernel of an operating system allows 
for multiple isolated user space instances, instead of just one. Such 
instances (often called containers, virtualization engines (VE), 
virtual private servers (VPS), or jails) may look and feel like a real 
server from the point of view of its owners and users. In addition to 
isolation mechanisms, the kernel often provides resource management 
features to limit the impact of one container's activities on the 
other containers. Container technology is currently employed as a way 
of reducing the complexity of software deployment and portability of 
applications in cloud computing infrastructure. Arguably, containers 
have taken the role that package management tools had in the past, 
where they were used to control upgrades and keep track of change in 
the dependencies of an application [@di_cosmo_package_2008].

In this work, we make the case for containers as a way of tackling 
some of the reproducibility problems in computer systems research. 
Specifically, we propose to use the resource accounting and limiting 
components of OS-level virtualization as a basis for creating 
execution profiles of experiments that can be associated with results, 
so that these can subsequently be analyzed when an experiment is 
evaluated. In order to reduce the problem space, we focus on local and 
distributed storage systems in various forms (e.g., local file 
systems, distributed file systems, key-value stores, and related 
data-storage engines) since this is one of the most important areas 
underlying cloud computing and big-data processing, as well as our 
area of expertise.

The rest of this paper is organized as follows. We first describe the 
distinct levels of reproducibility that can be associated with 
scientific claims in systems research and give concrete examples in 
the area of storage [systems](#experiment). We then analyze salient 
features of container technology that are relevant in the evaluation 
of experiments and introduce what in our view is missing in order to 
make containers a useful reproducibility tool for storage systems 
[research](#extension). We subsequently present a use case that 
illustrates the benefits of the proposed container management 
extensions that we [envision](#case). We then follow with a discussion 
about the benefits and limitations of using containers in other areas 
of experimental systems [research](#discussion). We finally discuss 
related [work](#related) and [conclude](#conclusion).

# Evaluation of Experimental Systems Research {#experiment}

An experiment in systems research is composed of a triplet of \(1\) 
workload, (2) a specific system where the workload runs and \(3\) 
results from a particular execution. Respective to this order is the 
complexity associated with the evaluation of an experiment: obtaining 
the exact same results is more difficult than just getting access to 
the original workload. Thus, we can define a taxonomy to characterize 
the reproducibility of experiments:

 1. _Workload Reproducibility_. We have access to the original code 
    and the particular workload that was used to obtain the original 
    experimental results.
 2. _System Reproducibility_. We have access to hardware and software 
    resources that resemble the original dependencies.
 3. _Results Reproducibility_. The results of the re-execution of an 
    experiment are valid with respect to the original.

In storage systems research, workload reproducibility is achieved by 
getting access to the configuration of the benchmarking tool that 
defines the I/O patterns of the experiment. For example, if an 
experiment uses the [Flexible I/O Tester][fio] (FIO), then the 
workload is defined by the FIO input file.

System reproducibility can be divided into software and hardware. The 
former corresponds to the entire software stack from the 
firmware/kernel up to the libraries used by an experiment. The latter 
comprises the set of hardware devices involved in the experiment such 
as the specific CPU model, storage drives or network cards on which 
the experiment ran.

Reproducing results does not necessarily imply the regeneration of the 
exact same measurements; instead it entails validating the results by 
checking how close (in shape or trends) to the original experiment 
they are. Given this, evaluating an experiment can be a subjective 
task. If our aim is to elevate our discipline to the same rank of 
other experimental sciences, evaluation of results should never be 
subjective. In systems research, result reproducibility depends on the 
particular goals of the experiment; within the domain of storage 
systems, we propose to evaluate results based on resource utilization 
metrics, specifically memory, CPU, and I/O bandwidth, to provide 
objective standards for unambiguously comparing results.

# Containers For Reproducible Systems Research {#extension}

Current implementations of OS-level virtualization (e.g. [LXC]  or 
[OpenVZ]) include an accounting component that keeps track of the 
resource utilization of a container over time. In general, this module 
can account for CPU, memory, network and I/O usage. By periodically 
checking and recording these metrics while an experiment runs, we can 
obtain a profile of its execution. This profile is the signature of 
the experiment on the particular hardware on which it executed. The 
challenge is to recreate results on distinct hardware. By coupling 
this execution profile to a profile of the underlying hardware, we can 
provide valuable information for researchers to use while evaluating a 
particular result. In concrete, when trying to reproduce an experiment 
on a system $B$ that originally ran on $A$, we propose the following 
mapping methodology:

  1. Obtain the hardware profile of $A$.
  2. Obtain the configuration of every container involved in the 
     experiment, along with their execution profile.
  3. Obtain the hardware profile of $B$.
  4. Generate a configuration for the experiment w.r.t. $B$ by 
     recreating the resource allocation that the experiment had when 
     it ran on $A$.

Using LXC as an example, we show in Figure 1 a monitoring daemon 
running as a userspace process in the host that implements the process 
described above.

![A userpace process running alongside the container execution engine 
(LXC) that periodically probes the statistics of containers in order 
to obtain an execution profile.](figures/monitor.png)

The hardware profile is composed of static (e.g. the hardware 
characteristics as seen by `lshw`) and dynamic information (e.g. the 
results of micro-benchmarks that characterize the bare-metal 
performance of a machine). The container configuration corresponds to 
the host's resources available to a container, such as number of CPUs 
and total amount of memory. While the experiment runs, performance 
metrics for the container can be obtained in order to create an 
execution profile.

The monitoring process periodically dumps the content of the `cgroups` 
pseudo-filesystem in order to capture the runtime metrics of 
containers running in the system. An alternative for structuring this 
information is by defining the following schema:

` (IMG|EX_ID|HW_PROFILE|CGROUPS|EX_PROFILE)`

Where `IMG` points to the image from where the container was 
instantiated. `EXE_ID` corresponds to a particular execution of the 
experiment with associated timestamps. `HW_PROFILE`, as mentioned 
above, captures the bare-metal performance of the machine where the 
container executes. `CGROUPS` is the configuration in terms of control 
groups for the distinct subsystems (CPU, memory, network and I/O). 
`EX_PROFILE` is the profile for a particular execution; i.e. there is 
one profile for every entry (one for each `EX_ID`).

The profile database can be located remotely in a central repository 
that serves as the hub for managing experiments in a distributed 
environment. For example, this monitoring component could be 
implemented as a submodule of CloudLab [@ricci_introducing_2014]. For 
experiments consisting of multiple hosts and container images, 
orchestration tools such as Mesos [@hindman_mesos_2011] can also be 
extended to incorporate this profiling functionality. One of main 
goals of our work is to determine whether this information is 
sufficient to evaluate a result.

# Use Case: Scalability Experiments of Ceph OSDI '06 {#case}

To illustrate the utility of having execution and hardware profiles, 
we take the Ceph OSDI '06 paper [@weil_ceph_2006] and reproduce one of 
its experiments. In particular, we look at the scalability experiment 
from the data performance section (6.1). The reason for selecting this 
paper is that we are familiar with these experiments. This makes it 
easier to reason about contextual information not necessarily 
available directly from the paper.

The experiments in Section 6.1 of the original paper showed the 
ability of Ceph to saturate disk evenly among the drives of the 
cluster. Figures 5-7 from the original paper showed per-OSD 
performance as the object size varied from 4 KB to 4 MB. Results of 
the scalability experiment are presented in Section 6.1.3 of the Ceph 
paper (Figure 8 on the original paper; reprinted below in Figure 2). 
The goal of this experiment is to show that Ceph scales linearly with 
the number of storage nodes, up to the point where the network switch 
is saturated. This linear scalability is our reproducibility 
evaluation criteria for this specific experiment.

![Reprinting Figure 8 from the original paper. The original caption 
reads: "_OSD write performance scales linearly with the 
size of the OSD cluster until the switch is saturated at 24 OSDs. 
CRUSH and hash performance improves when more PGs lower variance in 
OSD utilization_."](figures/figure8.png)

The experiment used 4 MB objects to minimize random I/O noise from the 
hard drives. We ignore the performance of the `hash` data distribution 
and increase the number of placement groups to 128 per node, thus we 
meaningfully compare against the red solid-dotted line in Figure 8 of 
the Ceph paper.

## Reproducing Results on Similar Hardware

A subset of the hardware used for the Ceph experiments is still 
available in our laboratory. Each node in the system consist of a 
2-core 2212 AMD Opteron @2.0GHz, 8GB of RAM, 1GbE NIC and 250GB 
Seagate Barracuda ES hard drives. We created a containerized version 
of the experiment using the 0.87 branch of Ceph. We use docker 1.3.3 
and LXC 1.0.6 running on Ubuntu 12.04 hosts (3.13.0-43 x86_64 kernel).

The original scalability experiment ran with 20 clients per node on 20 
nodes (400 clients total) and varied the number of OSDs from 2-26 in 
increments of 2. Every node was connected via 1 GbE link, so the 
experiment theoretical upper bound was 2GB/s (when there was enough 
capacity of the OSD cluster to have 20 1Gb connections) or 
alternatively when the connection limit of the switch was reached. The 
paper experiments were executed on a Netgear switch. This device has a 
capacity of approximately 14 GbE in _real_ total traffic (from a 20 
advertised), corresponding to the 24 * 58 = 1400 MB/s combined 
throughput shown in the original paper.

We scaled down the experiment by reducing the number of client nodes 
to 1 (running 16 client threads). This means that our network upper 
bound is approximately 110 MB/s (the capacity of the 1GbE link from 
the client to the switch). We throttle I/O at 30 MB/s, so this is our 
scaling unit (the per-OSD increment). The reason for throttling at 30 
MB/s is that, over time, the Seagate disks have aged (they are 10 
years old!) and overall performance among the hard drives of our 
cluster is different from the ~58 MB/s observed in the original paper. 
In order to amortize, we had to take the lowest common denominator 
which in this case is 30 MB/s. We throttle I/O by configuring LXC 
containers with the control group `blkio.throttle.write_bps_device` 
directive. Figure 3 shows results of this scaled-down, throttled 
version of the scalability experiment. An open question is to 
determine if the process of scaling-down and throttling resources can 
be automated, given the profile repository described in the previous 
section.

![Reproducing a scaled-down version of the original OSDI '06 
scalability experiment. The y-axis represents average throughput, as 
seen by the client. They x-axis corresponds to the size of the cluster 
(in number of object storage devices (OSD). The square marker 
corresponds to the average of 10 executions. The line with triangle 
markers projects the original results to our setting. This projection 
is obtained by having the 58 MB/s divided by 2 (to reflect the doubled 
I/O operation of the current Ceph version), i.e. 24 MB/s as the 
scalability unit of the original experiment.](figures/ceph1.png)

We see that Ceph scales linearly with the number of OSDs, up to the 
point where we saturate the 1GbE link[^linearity]. We note that we 
don't see 30 MB/s of net I/O utilization since the current version of 
Ceph issues two I/O calls on each write request, one to the 
write-ahead log and another one to the data backend. The original 
experiments used a prototype version of Ceph that didn't include this 
atomicity/durability feature. Figure 3 also shows a projection of the 
original data to our setting. The original result shows better 
scalability behavior due to newer and more stable hard drives.

[^linearity]: The experiment scales linearly up to 4 nodes. At OSD 
number 5, the 1 GbE link begins to exhibit the effects of network 
pressure. We empirically corroborated this by re-executing the 
experiment with two client nodes, in which case the experiment scales 
linearly up to 8 OSD nodes; at OSD number 9, the two links begin to be 
pressured (approximately 140 MB/s). This data is available at the 
repository associated to this paper (see _Section V.C_).

## Reproducing Results on Different Hardware

So far we have discussed how to reproduce an experiment on the 
original hardware. But, as we have mentioned before, the challenge is 
in reproducing experiments on different hardware. Having the 
experiment implemented in containers allows us to swap components of 
the underlying hardware and repeat the experiment easily; after all, 
this is one of the ultimate goals of virtualization, and containers 
aim at doing it with minimal overhead. Our interest is in measuring 
the effects that replacing distinct components has on experimental 
results. Our conjecture is that, for many cases, the mapping 
methodology defined in the previous section will allow to reproduce 
results on distinct hardware. As part of our efforts, we are working 
in characterizing the cases for which our methodology will work and 
those for which it won't.

Thus, one of our initial goals is to empirically test the 
repercussions of replacing storage, CPU, memory and network devices 
(among others). We now present preliminary results on the outcome of 
swapping distinct storage drives. We re-executed the scalability 
experiment, swapping four old hard drives with newer models. The 
results are shown in Figure 4.

![Showing the effect of replacing 4 hard drives with newer models. Old 
hard drives are the same used in the previous figure and correspond to 
a set of 10 year old 250GB Seagate Barracuda ES (ST3250620NS). New 
hard drives correspond to 500GB Western Digital Re (WD5003ABYZ) 
drives. Every data point corresponds to the average (and standard 
error) of 10 executions.](figures/ceph2.png)

The newer hard drives have the capacity to write at ~130 MB/s but we 
throttle I/O in order to replicate the behavior of our older drives. 
Standard error markers show that differences for two of the data 
points are statistically significant. Our expectation was to find 
complete overlapping points, since at this scalability levels (1-4), 
variance is relatively low. Additionally, the `blkio` cgroups 
subsystem has been empirically shown to effectively isolate I/O 
operations at low loads [@sfakianakis_vanguard_2014].

After investigating further about the reason of these differences, we 
found the following. As mentioned earlier, Ceph issues two I/O calls 
on each write request, one of them being asynchronous. The cgroups 
`blkio` controller responsible for limiting I/O on block devices 
(which we configure to 30 MB/s) cannot throttle asynchronous I/O 
operations since this type of requests go to a queue that is shared at 
the system level by all containers running in the host. We 
experientially corroborated this by executing a microbenchmark using 
FIO that executed the same load (4MB files) on the two hard drives in 
question, but using direct I/O exclusively. In this case, the 
performance corresponds to the throttled 30 MB/s (lines perfectly 
overlap). We then executed a mixed workload of both direct and async 
I/O requests and observed that the newer hard drive performs better 
than the old one, with similar results as those showed in Figure 4.

# Discussion {#discussion}

We discuss other benefits and limitations of containerization, as well 
as general reproducibility guidelines when working with containers.

## Cataloging Experiments

By storing performance profiles of experiments and associated 
hardware, we can create categories of container metrics that describe 
in a high-level what the experiment's goal is, for example:

  * In-memory only
  * Storage intensive
  * Network intensive
  * CPU intensive
  * 50% of caching effects

Assuming there is a central repository of experiments associated with 
infrastructures such as CloudLab [@ricci_introducing_2014]. A 
scientist wanting to test new ideas in systems research can look for 
experiments in this database by issuing queries with a particular 
category in mind.

## Can All Systems Research Be Containerized? {#all}

Based on previous performance evaluations of container technology, in 
particular LXC [@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014], we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of extreme importance (i.e. if 5% of 
    performance will affect results). Some experiments show a 
    significant overhead of the memory subsystem while handling the 
    limits imposed to containers.
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located. For example, AUFS has been shown to incur a 
    penalty when processes write to the container's filesystem.
  * Network address translation (NAT) is required.
  * Distinct experiments consolidated on the same host. Even though 
    many containers can be co-located in the same host, isolating and 
    accounting can affect the performance of the server host. This can 
    be minimized by placing as few containers as possible on the same 
    host.
  * Kernel version can't be pinned to a particular version. Since the 
    OS is the hypervisor, distinct kernel versions observe distinct 
    performance characteristics.

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects. This list is not comprehensive, of course, and an open 
problem is to delineate precisely the boundaries between experiments 
well-suited to be reproducible via containers vs. those that are not.

## Other Lessons Learned So Far

We list some of the lessons that we have learned as part of our 
experience in implementing experiments in containers:

  * Version-control the experiment's code and its dependencies, 
    leveraging git subtrees/submodules (or alike) to keep track of 
    inter-dependencies between projects. For example, if a git 
    repository contains the definition of a Dockerfile, make it a 
    submodule of the main project.
  * Refer to the specific version ID that a paper's results were 
    obtained from. Git's tagging feature can also be used to point to 
    the version that contains the codebase for an experiment (e.g. 
    "`osdi_14`").
  * When possible, add experimental results as part of the commit that 
    contains the codebase of an experiment. In other words, try to 
    make the experiment as self-contained as possible, so that 
    checking out that particular version contains all the dependencies 
    and generated data.
  * Keep a downloadable container image for the version of the 
    experiment codebase (e.g. use the docker registry and its 
    automated build feature).
  * Whenever possible, use continuous integration (CI) technologies to 
    ensure that changes to the codebase don't disrupt the 
    reproducibility of the experiment.
  * Obtain a profile of the hardware used (eg. making use of tools 
    such as [SoSReport]), as well as the resource configuration of 
    every container and publish these as part of the experimental 
    results (i.e. add it to the commit that a paper's results are 
    based on).

We followed this guidelines for this paper. All the resources needed 
to generate its content can be found in our github 
[account](https://github.com/ivotron/woc_2015#camera-ready).

# Related Work {#related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_computational_2012 ; 
@neylon_changing_2012 ; @leveqije_reproducible_2012 ; 
@stodden_implementing_2014], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave runtime 
performance as a secondary problem. In systems research, runtime 
performance _is_ the subject of study, thus we need to look at it as a 
primary issue. By obtaining profiles of executions and making them 
part of the results, we allow researchers to validate experiments with 
performance in mind.

In [@collberg_measuring_2014] the authors took 613 articles published 
in 13 top-tier systems research conferences and found that 25% of the 
articles are reproducible (under their reproducibility criteria). The 
authors did not analyze performance. In our case, we are interested 
not only in being able to rebuild binaries and run them but also in 
evaluating the performance characteristics of the results.

Containers, and specifically docker, have been the subject of recent 
efforts that try to alleviate some of the reproducibility problems in 
data science [@boettiger_introduction_2014]. Existing tools such as 
Reprozip [@chirigati_reprozip_2013] package an experiment in a 
container without having to initially implement it in one (i.e. 
automates the creation of a container from an "non-containerized" 
environment). Our work is complementary in the sense that we look at 
the conditions in which the experiment can be validated in terms of 
performance behavior if it runs within a container.

# Conclusion {#conclusion}

In this paper we have presented our proposal for complementing 
container management infrastructure to capture execution profiles with 
the purpose of making these available to experimental research 
reviewers and readers. We illustrated the benefits of our hardware and 
container execution profiling methodology by reproducing a previously 
published experiment in the area of distributed storage systems. We 
are currently in the process of further testing these ideas on 
experiments from other areas of systems research such as kernel 
development and network systems.

<!--
**Acknowledgements:** This work was partially supported by NSF grant 
... We wish to thank ...
-->

# References

[SoSReport]: https://www.github.com/sosreport/sos
[fio]: https://github.com/axboe/fio
[LXC]: https://www.kernel.org/doc/Documentation/cgroups
[OpenVZ]: https://wiki.openvz.org/Proc/user_beancounters

<!-- hanged biblio -->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}


