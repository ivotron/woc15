---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez"
    affiliation: "UC Santa Cruz"
    email: "ivo@cs.ucsc.edu"
  - name: "Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "carlosm@ucsc.edu"
  - name: "Adam Moody"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "moody11@llnl.gov"
  - name: "Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "kathryn@llnl.gov"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "gflofst@sandia.gov"
  - name: "Remzi Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "remzi@cs.wisc.edu"
abstract: |
  Evaluating experimental results in the field of Computer Systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we analyze salient features of container technology 
  that, if leveraged correctly, can help reducing the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of storage systems to illustrate the extensions that we 
  envision, mainly in terms of container management infrastructure. We 
  also discuss the benefits and limitations of using containers as a 
  way of reproducing research in other areas of experimental systems 
  research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
substitute-hyperref: true
documentclass: acm-sig-alternate
disable-copyright: true
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous results. Registering information about an 
experiment (referred to as _provenance_) such as annotations, diagrams 
and lists, allows scientists to interpret and understand results. By 
examining the steps that led to a result, scientists can gain insights 
into the chain of reasoning used in its production, verify that the 
experiment was performed according to acceptable procedures, identify 
the experiment's inputs, and, in some cases, regenerate the original 
result [@freire_provenance_2008]. Additionally, reproducibility plays 
a major roll in education since the amount of information that a 
student has to digest increases as the pace of scientific discovery 
accelerates. By having repeatable experiments, a student can learn by 
looking at provenance information, re-evaluate the questions that the 
original experiment answered and thus "stand in the shoulder of 
giants".

In applied Computer Science an experiment is carried out in its 
entirety by a computer [@freire_computational_2012 ; 
@donoho_reproducible_2009]. Although the scientist is interactively 
defining the sequence of steps that get executed, ultimately binary 
programs run on a machine. Repeating a result doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`). With our current 
computational abilities to collect and manage data we should be able 
to record and reconstruct environments for particular results 
perfectly. However, this is not the case, mainly due to three factors:

  1. Availability of an experiment's dependencies[^andalso].
  2. Evolving software.
  3. Changes in hardware.

Version control systems (VCS) are sometimes used to tackle 1 and 2. By 
having a particular version ID for the software used for an article's 
experimental results, reviewers and readers can have access to the 
same codebase [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_measuring_2014] 
since the code might not compile and, even if compilable, the results 
might differ, in which case the differences have to be analyzed in 
order to corroborate the validity of the original experiment.

[^andalso]: By dependencies we mean the codebase and all other assets 
associated to results, such as input/output files and library 
dependencies.

Reproducing experimental results when the underlying hardware 
environment changes (3) is challenging mainly due to the inability of 
predicting the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads associated in terms of performance (the 
hypervisor "tax") and management (creating, storing and transferring 
them) can be high and, in some fields of Computer Science such as 
Computer Systems research, cannot be accounted easily 
[@clark_xen_2004].

Container technology [@soltesz_container-based_2007] is currently 
employed as a way of reducing the complexity of software deployment 
and portability of applications in cloud computing infrastructure. 
Arguably, containers have taken the role that package management tools 
had in the past, where they were used to control upgrades and keep 
track of change in the dependencies of an application 
[@di_cosmo_package_2008]. In this work, we make the case for 
containers as a way of tackling some of the reproducibility problems 
in Computer Systems research. In order to reduce the problem space, we 
focus to local and distributed storage systems in various forms (e.g., 
local file systems, distributed file systems, key-value stores, and 
related data-storage engines) since this is one of the most important 
areas underlying cloud computing and big-data processing, as well as 
our area of expertise.

The rest of this paper is organized as follows. We first describe the 
distinct levels of reproducibility that can be associated with 
scientific claims in systems research and give concrete examples in 
the area of storage [systems](#experiment). We then analyze salient 
features of container technology that are relevant in the evaluation 
of experiments and introduce what in our view is missing in order to 
make containers a useful reproducibility tool for storage systems 
[research](#extension). We subsequently present a use case that 
illustrates the container management extensions that we 
[envision](#case). We finally discuss some limitations of using 
containers in other areas of experimental systems 
[research](#discussion) and present [related](#related) and future 
[work](#future).

# Experimental Evaluation of Computer Systems Research {#experiment}

An experiment in systems research is usually composed by a triplet of 
\(1\) workload, (2) a specific system where the workload runs and 
\(3\) results from a particular execution. Respective to this order is 
the complexity associated to the evaluation of an experiment: 
obtaining the exact same results is more difficult than just getting 
access to the original workload. Thus, we can define a taxonomy to 
characterize the reproducibility of experiments:

 1. _Workload Reproducibility_. We have access to the original code 
    and the particular workload that was used to obtain the original 
    experimental results.
 2. _System Reproducibility_. We have access to hardware and software 
    resources that resemble the original dependencies.
 3. _Results Reproducibility_. The results of the re-execution of an 
    experiment are valid with respect to the original.

In storage systems research, workload reproducibility usually entails 
getting access to the configuration of the benchmarking tool that 
defines the IO patterns of the experiment. For example, if an 
experiment uses the Flexible IO Tester[^fio] (FIO), then the workload 
is defined by the FIO input file.

System reproducibility can be divided in software and hardware. The 
former corresponds to the entire software stack from the 
firmware/kernel up to the libraries used, whereas the latter comprises 
the same set of hardware devices involved in the experiment such as 
specific storage drives and network switches and cards.

Reproducing results does not necessarily imply the re-generation of 
the exact same measurements, instead it entails validating the results 
by checking how close (in shape or trends) to the original experiment 
they are. Given this, evaluating an experiment can be a subjective 
task. We propose specific definitions within the domain of storage 
systems in terms of resource utilization, for example, bandwidth 
(IOPS) of a storage device.

While container technology can be used to achieve 1 and partially 2 
(thanks to the way images are snapshots of the entire file system), 
the challenge is in the evaluation of results when the underlying OS 
kernel and hardware change (3), which is the subject of the next 
section.

# Containers For Reproducible Systems Research {#extension}

For experiments suitable to be containerized ([see](#all) for a 
discussion), we define a mapping methodology that allows to evaluate 
experimental results on hardware distinct than the original:

  1. Obtain the profile of the original hardware
  2. Obtain the configuration of each container (e.g. User Beacons or 
     cgroup configuration)
  3. Obtain the profile of the new hardware
  4. Generate a configuration for the new hardware based on 2-4

We envision the following piece of infrastructure [^note]:

  0. Hardware profiling module that runs at multiple intervals (e.g. 
     daily, for every execution)
  1. A monitoring process on each host OS
  2. Notion of distributed execution of an experiment
  3. Monitor daemon keeps track of container metrics
  4. At the end of execution.

[^note]: **NOTE:** for the camera-ready version of this paper we will 
add a diagram of the architectural view of our extensions.

# Use Case: Reproducing Results on Different Storage Devices {#case}

We compare:

  * An old hard drive
  * The equivalent (modern) model of an old hard drive
  * An SSD

We will show that:

  * Direct I/O is reproducible
  * Asynchronous I/O is not
  * Therefore we need to extend containers to handle async requests on 
    a per-container basis (not on a global)

This case will also illustrate how having the HW profile and base 
configuration of containers can help in:

  * mapping to new hardware
  * mapping to old-but-aged hardware

# Discussion {#discussion}

We discuss other benefits and limitations of containerization, as well 
as general reproducibility guidelines when working with containers.

## Cataloging Experiments

We can characterize experiments by creating categories of container 
metric profiles, e.g.:

  * In-memory only
  * Storage intensive
  * Network intensive
  * CPU intensive
  * xx% of caching effects

We need to profile an experiment, store the profile along with the 
image of the container, categorize the profile and index it.

## Can All Systems Research Be Containerized? {#all}

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * Distinct experiments consolidated on the same host.
  * For an experiment, containers with conflicting roles need to be 
    co-located in the same physical host (e.g. two database instances 
    on the same host).
  * Kernel version can't be frozen.

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects.

## Other Lessons Learned So Far

We list some of the lessons that we have learned as part of our 
experience in implementing experiments in containers:

  * Version control the experiment's code and its dependencies, 
    leveraging git subtrees/submodules (or alike) to keep track of 
    inter-dependencies between projects. For example, if a git 
    repository contains the definition of a Dockerfile, make it a 
    submodule of the main project.
  * Refer to the specific version ID that a paper's results were 
    obtained from. Git's tagging feature can also be used to point to 
    the version that contains the codebase for an experiment (e.g. 
    "sosp14").
  * When possible, add experimental results as part of the commit that 
    contains the codebase of an experiment. In other words, try to 
    make the experiment as self-contained as possible, so that 
    checking out that particular version contains all the dependencies 
    and generated data.
  * Keep a downloadable container image for the version of the 
    experiment codebase (e.g. use the docker registry and its 
    automated build feature).
  * Whenever possible, use CI technologies to ensure that changes to 
    the codebase don't disrupt the reproducibility of the experiment.
  * Obtain a profile of the hardware used (eg. making use of tools 
    such as SoSReport[^sosreport]), as well as the resource 
    configuration of every container and publish these as part of the 
    experimental results (i.e. add it to the commit that a paper's 
    results are based on).

# Related Work {#related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_provenance_2008 ; 
@freire_computational_2012 ; @stodden_implementing_2014 ; 
@neylon_changing_2012 ; @cheney_provenance_2009 ; 
@leveqije_reproducible_2012], where the focus is more on numerical 
reproducibility rather than performance. Similarly, efforts such as 
_The Recomputation Manifesto_ [@gent_recomputation_2013] and the 
_Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave runtime 
performance as a secondary problem. In systems research, runtime 
performance _is_ the subject of study, thus we need to look at it as a 
primary issue. By obtaining profiles of executions and making them 
part of the results, we allow researchers to validate experiments with 
performance in mind.

In [@collberg_measuring_2014], the authors took 613 articles published 
in 13 top-tier systems research conferences and found that 25% of the 
articles are reproducible (under their reproducibility criteria). The 
authors did not analyzed performance. In our case, we are interested 
not only in being able to rebuild binaries and run them but also in 
evaluating the results.

Containers, and specifically docker, have been the subject of recent 
efforts that try to alleviate some of the reproducibility problems in 
data science [@boettiger_introduction_2014]. Existing tools such as 
Reprozip [@chirigati_reprozip_2013] package an experiment in a 
container without having to initially implement it in one (i.e. 
automates the creation of a container from an "non-containerized" 
environment). Our work is complementary in the sense that we look at 
the conditions in which the experiment can be validated in order tu 
ensure that the containerized version will (or won't) produce valid 
results.

# Conclusion and Future Work {#future}

Take papers from other sub-disciplines of Systems Research:

  * Kernel
  * Network
  * Hardware

How can we repeat the effort presented in this paper for the above?

# Acknowledgements

This work was partially supported by NSF grant ... We wish to thank 
...

# References

[^sosreport]: <https://www.github.com/sosreport/sos>

[^recomp]: <http://www.recomputation.org/manifesto>

[^fio]: <https://github.com/axboe/fio>

<!-- hanged biblio-->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
