---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez, Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "`(ivo|carlosm)@cs.ucsc.edu`"
  - name: "Adam Moody, Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "`(moody11|kathryn)@llnl.gov`"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "`gflofst@sandia.gov`"
  - name: "Remzi Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "`remzi@cs.wisc.edu`"
abstract: |
  Evaluating experimental results in the field of Computer Systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we analyze salient features of container technology 
  that, if leveraged correctly, can help reducing the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of storage systems to illustrate the extensions that we 
  envision, mainly in terms of container management infrastructure. We 
  also discuss the benefits and limitations of using containers as a 
  way of reproducing research in other areas of experimental systems 
  research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
links-as-notes: true
substitute-hyperref: true
no-page-number-on-titlepage: true
documentclass: ieeetran
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous results. Registering information about an 
experiment allows scientists to interpret and understand results, as 
well as verifying that the experiment was performed according to 
acceptable procedures. Additionally, reproducibility plays a major 
roll in education since the amount of information that a student has 
to digest increases as the pace of scientific discovery accelerates. 
By having repeatable experiments, a student can learn by looking at 
provenance information, re-evaluate the questions that the original 
experiment answered and thus "stand in the shoulder of giants".

In applied Computer Science an experiment is carried out in its 
entirety by a computer. Repeating a result doesn't require a scientist 
to rewrite a program, rather it entails obtaining the original program 
and executing it (possibly in a distinct environment). Thus, in 
principle, a well documented experiment should be repeatable 
automatically (e.g. by typing `make`), however, this is not the case. 
Today's computational environments are complex and accounting for all 
possible effects of changes within and across systems is a challenging 
task [@freire_computational_2012 ; @donoho_reproducible_2009].

Version control systems (VCS) are sometimes used to address some of 
these problems. By having a particular version ID for the software 
used for an article's experimental results, reviewers and readers can 
have access to the same codebase [@brown_how_2014]. However, 
availability of the source code does not guarantee reproducibility 
[@collberg_measuring_2014] since the code might not compile and, even 
if compilable, the results might differ, in which case the differences 
have to be analyzed in order to corroborate the validity of the 
original experiment.

Additionally, reproducing experimental results when the underlying 
hardware environment changes challenging mainly due to the inability 
of predicting the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads associated in terms of performance (the 
hypervisor "tax") and management (creating, storing and transferring 
them) can be high and, in some fields of Computer Science such as 
Computer Systems research, cannot be accounted easily [@clark_xen_2004].

Container technology [@soltesz_container-based_2007] is currently 
employed as a way of reducing the complexity of software deployment 
and portability of applications in cloud computing infrastructure. 
Arguably, containers have taken the role that package management tools 
had in the past, where they were used to control upgrades and keep 
track of change in the dependencies of an application 
[@di_cosmo_package_2008]. In this work, we make the case for 
containers as a way of tackling some of the reproducibility problems 
in Computer Systems research. Specifically, we propose to use the 
resource accounting and limiting components of OS-level virtualization 
as a basis for creating execution profiles of experiments that can be 
associated with results, so that these can subsequently be analyzed 
when an experiment is evaluated. In order to reduce the problem space, 
we focus on local and distributed storage systems in various forms 
(e.g., local file systems, distributed file systems, key-value stores, 
and related data-storage engines) since this is one of the most 
important areas underlying cloud computing and big-data processing, as 
well as our area of expertise.

The rest of this paper is organized as follows. We first describe the 
distinct levels of reproducibility that can be associated with 
scientific claims in systems research and give concrete examples in 
the area of storage [systems](#experiment). We then analyze salient 
features of container technology that are relevant in the evaluation 
of experiments and introduce what in our view is missing in order to 
make containers a useful reproducibility tool for storage systems 
[research](#extension). We subsequently present a use case that 
illustrates the container management extensions that we 
[envision](#case). We then follow with a discussion about the benefits 
and limitations of using containers in other areas of experimental 
systems [research](#discussion). We finally discuss related 
[work](#related) and [conclude](#conclusion).

# Evaluation of Experimental Systems Research {#experiment}

An experiment in systems research is composed by a triplet of \(1\) 
workload, (2) a specific system where the workload runs and \(3\) 
results from a particular execution. Respective to this order is the 
complexity associated to the evaluation of an experiment: obtaining 
the exact same results is more difficult than just getting access to 
the original workload. Thus, we can define a taxonomy to characterize 
the reproducibility of experiments:

 1. _Workload Reproducibility_. We have access to the original code 
    and the particular workload that was used to obtain the original 
    experimental results.
 2. _System Reproducibility_. We have access to hardware and software 
    resources that resemble the original dependencies.
 3. _Results Reproducibility_. The results of the re-execution of an 
    experiment are valid with respect to the original.

In storage systems research, workload reproducibility is achieved by 
getting access to the configuration of the benchmarking tool that 
defines the IO patterns of the experiment. For example, if an 
experiment uses the [Flexible IO Tester][fio] (FIO), then the workload 
is defined by the FIO input file.

System reproducibility can be divided in software and hardware. The 
former corresponds to the entire software stack from the 
firmware/kernel up to the libraries used by an experiment, whereas the 
latter comprises the set of hardware devices involved in the 
experiment such as specific the CPU model, storage drives or network 
cards that an experiment ran on.

Reproducing results does not necessarily imply the regeneration of the 
exact same measurements, instead it entails validating the results by 
checking how close (in shape or trends) to the original experiment 
they are. Given this, evaluating an experiment can be a subjective 
task. We propose metrics within the domain of storage systems in terms 
of resource utilization, specifically memory, CPU, and bandwidth, as a 
way of having objective metrics that do not give rise to ambiguity 
while comparing results.

# Containers For Reproducible Systems Research {#extension}

Current implementations of OS-level virtualization (e.g. [LXC]  or 
[OpenVZ]) include an accounting component that keeps track of the 
resource utilization of a container over time. In general, this module 
can account for CPU, memory, network and IO. By periodically checking 
and recording these metrics while an experiment runs, we can obtain a 
profile of its execution. This profile is the signature of the 
experiment on the particular hardware that it was executed on. The 
challenge is to recreate results on distinct hardware. By having the 
performance profile along with the performance profile of the 
underlying hardware, we can provide enough information for researches 
to use while evaluating a particular result. In concrete, we propose 
the following mapping methodology:

  1. Obtain the profile of the original hardware
  2. Obtain the resource utilization configuration of each container
  3. Obtain the profile of the new hardware
  4. Generate a configuration for the new hardware based on 2-4

The hardware profile is composed of static (e.g. the output of `lshw`) 
and dynamic information (e.g. the execution of micro-benchmarks to 
characterize the bare-metal performance of a machine).

![A userpace process running alongside the container manager (LXC) 
that periodically probes the statistics of containers in order to 
obtain an execution profile.](/root/figures/monitor.png)

Using LXC as an example, we show in Figure 1 a monitoring daemon 
running in as a userspace process in the host. This process 
periodically dumps the content of the `cgroups` pseudo-filesystem in 
order to capture the runtime metrics of the containers running in the 
system.

An alternative for structuring this information is by defining the 
following schema: `(IMAGE ID | EXECUTION ID | HW PROFILE ID | CGROUPS 
CONF | EXECUTION PROFILE)`. Where `IMAGE ID` corresponds to the image 
where the container was instantiated from. `EXECUTION ID` corresponds 
to a particular execution of the experiment with associated 
timestamps. `HW PROFILE ID`, as mentioned above, captures the 
bare-metal specification of the machine where the container executes.

The execution database can be located remotely in a central repository 
that serves as the hub for managing experiments in a distributed 
environment. For example, this monitoring component could be 
implemented as a submodule of CloudLab [@ricci_introducing_2014]. For 
experiments consisting of multiple hosts and container images, 
orchestration tools such as Mesos [@hindman_mesos_2011] can also be 
extended to incorporate this profiling functionality.

# Use Case: Scalability Experiments of Ceph OSDI '06 {#case}

To illustrate the utility of having execution and base hardware 
profiles, we take the Ceph OSDI '06 paper [@weil_ceph_2006] and 
reproduce one of its experiments. In particular, we look at the 
scalability experiment from the data performance section (6.1). The 
reason for selecting this paper is (1) our familiarity with these 
experiments makes it easier to reason about contextual information not 
necessarily available directly from the paper and (2) the original 
hardware used is still available in our laboratory. Each node in the 
system consist of an 2-core 2212 AMD Opteron @2.0GHz, 8GB of RAM, 1GbE 
NIC and 500GB Seagate Barracuda ES 250GB hard drives. We created a 
containerized version of the experiment using the 0.87 branch of Ceph. 
We use docker 1.3.3 and LXC 1.0.6 running on Ubuntu 12.04 hosts 
(3.13.0-43 x86_64 kernel).

The experiments on Section 6.1 show the ability of Ceph to saturate 
disk evenly among the drives of the cluster. Figures 5-7 from the 
original paper show per-OSD performance as the object size varies from 
4 KB to 4 MB. In our case we focus on reproducing the scalability 
experiment of Section 6.1.3 (Figure 8), which uses 4 MB objects, to 
avoid random IO noise from the hard drives. We ignore the performance 
of `hash` data distribution and fix the number of placement groups to 
32K, thus we can meaningful comparisons against the red solid-dotted 
line in Figure 8.

The original scalability experiment ran with 20 clients per node on 20 
nodes (400 clients total) and varies the number of OSDs from 2-26 in 
increments of 2. Every node was connected via 1 GbE link, so the 
experiment theoretical upper bound is 2GB/s (when there is enough 
capacity of the OSD cluster to have 20 1Gb connections) or 
alternatively when the connection limit of the switch is reached. The 
paper experiments where executed on a Netgear switch which has a 
capacity of approximately 14 GbE in _real_ total traffic (from a 20 
advertised), which corresponds to the 24 * 58 = 1400 MB/s combined 
throughput shown in the original paper.

We scaled down the experiment by reducing the number of clients to 1 
node (16 clients). This means that our network upper bound is 110 MB/s 
(the capacity of the 1GbE link from the client to the switch). We 
throttle IO at 30 MB/s (vs. 58 MB/s of the original paper), so this is 
our scaling unit (per OSD increment). The reason for scaling down to 
30 MB/s is that over time, the Seagate disks have aged and performance 
profile among the hard drives of our cluster is different from the 
original ~58 MB/s observed in the original paper. In order to 
amortize, we had to take the lowest common denominator which in this 
case is 30 MB/s. Given this scaling, we obtain the results shown in 
Figure 2. We see that the Ceph scales linearly with the number of 
OSDs, up to the point where we saturate the 1GbE link.

![Reproducing a scaled-down version of the original OSDI '06 
scalability experiment (Figure 8). The y-axis represents average 
throughput, as seen by the client. They x-axis corresponds to the size 
of the cluster (in number of object storage devices (OSD). The dot 
corresponds to the average of 10 executions of the experiment. It also 
represents the overall performance of the hard disks in the cluster. 
Standard error bars show low variability.](/root/figures/ceph1.png)

<!--
The way in which we throttle I/O is by using the `blkio`...
-->

## Reproducing on Distinct Hardware

So far we have discussed how we are able to reproduce the experiment 
on the original hardware. As we have mentioned before, the challenge 
is in reproducing experiments on different hardware. In order to 
illustrate the benefits of using containers and their limiting 
resource utilization capabilities, we executed the same experiment by 
swapping 4 hard drives with newer models. The results are shown in 
Figure 3.

![Showing the effect of replacing 4 hard drives with newer 
models.](/root/figures/ceph2.png)

The reason for the change in results with respect to the first four 
points of Figure 2 (above) is the following. Ceph issues two I/O calls 
on each write request, one of them being asynchronous. The blkio 
controller responsible for limiting I/O on block devices (which we 
configure to 30MB/s) cannot throttle asynchronous I/O since this type 
of requests go to a queue that is shared at the OS level (among all 
containers). We empirically corroborated this by executing a small 
microbenchmark using FIO that executed the same load (4MB files) on 
the same two hard drives in question, but using direct I/O 
exclusively. In this case, the performance corresponds to the 
throttled 30 MB/s. We then executed a mixed workload of both direct 
and async I/O requests and observed that the newer hard drive performs 
better than the old one.

# Discussion {#discussion}

We discuss other benefits and limitations of containerization, as well 
as general reproducibility guidelines when working with containers.

## Cataloging Experiments

By storing profiles of experiments, we can create categories of 
container metrics that describe in a high-level what the experiment's 
goal is, for example:

  * In-memory only
  * Storage intensive
  * Network intensive
  * CPU intensive
  * 50% of caching effects

Assume there is a central repository of experiments such as CloudLab 
[@ricci_introducing_2014]. A scientist wanting to test new ideas in 
systems research can look for experiments in this database by issuing 
queries with a particular category in mind.

## Can All Systems Research Be Containerized? {#all}

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * Distinct experiments consolidated on the same host.
  * For an experiment, containers with conflicting roles need to be 
    co-located in the same physical host (e.g. two database instances 
    on the same host).
  * Kernel version can't be frozen.

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects.

## Other Lessons Learned So Far

We list some of the lessons that we have learned as part of our 
experience in implementing experiments in containers:

  * Version control the experiment's code and its dependencies, 
    leveraging git subtrees/submodules (or alike) to keep track of 
    inter-dependencies between projects. For example, if a git 
    repository contains the definition of a Dockerfile, make it a 
    submodule of the main project.
  * Refer to the specific version ID that a paper's results were 
    obtained from. Git's tagging feature can also be used to point to 
    the version that contains the codebase for an experiment (e.g. 
    "sosp14").
  * When possible, add experimental results as part of the commit that 
    contains the codebase of an experiment. In other words, try to 
    make the experiment as self-contained as possible, so that 
    checking out that particular version contains all the dependencies 
    and generated data.
  * Keep a downloadable container image for the version of the 
    experiment codebase (e.g. use the docker registry and its 
    automated build feature).
  * Whenever possible, use CI technologies to ensure that changes to 
    the codebase don't disrupt the reproducibility of the experiment.
  * Obtain a profile of the hardware used (eg. making use of tools 
    such as [SoSReport]), as well as the resource configuration of 
    every container and publish these as part of the experimental 
    results (i.e. add it to the commit that a paper's results are 
    based on).

# Related Work {#related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_provenance_2008 ; 
@freire_computational_2012 ; @stodden_implementing_2014 ; 
@neylon_changing_2012 ; @cheney_provenance_2009 ; 
@leveqije_reproducible_2012], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave runtime 
performance as a secondary problem. In systems research, runtime 
performance _is_ the subject of study, thus we need to look at it as a 
primary issue. By obtaining profiles of executions and making them 
part of the results, we allow researchers to validate experiments with 
performance in mind.

In [@collberg_measuring_2014], the authors took 613 articles published 
in 13 top-tier systems research conferences and found that 25% of the 
articles are reproducible (under their reproducibility criteria). The 
authors did not analyze performance. In our case, we are interested 
not only in being able to rebuild binaries and run them but also in 
evaluating the performance characteristics of the results.

Containers, and specifically docker, have been the subject of recent 
efforts that try to alleviate some of the reproducibility problems in 
data science [@boettiger_introduction_2014]. Existing tools such as 
Reprozip [@chirigati_reprozip_2013] package an experiment in a 
container without having to initially implement it in one (i.e. 
automates the creation of a container from an "non-containerized" 
environment). Our work is complementary in the sense that we look at 
the conditions in which the experiment can be validated in terms of 
performance behavior if it runs within a container.

# Conclusion {#conclusion}

In this paper we have presented our proposal for complementing 
container management infrastructure to capture execution profiles with 
the purpose of making these available to experimental research 
reviewers and readers. We are in the process of testing this ideas 
with concrete published papers in several areas of systems research.

**Acknowledgements:** This work was partially supported by NSF grant 
... We wish to thank ...

# References

[SoSReport]: https://www.github.com/sosreport/sos
[fio]: https://github.com/axboe/fio
[LXC]: https://www.kernel.org/doc/Documentation/cgroups
[OpenVZ]: https://wiki.openvz.org/Proc/user_beancounters

<!-- hanged biblio-->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}


