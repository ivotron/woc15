---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez, Carlos Maltzahn"
    affiliation: "University of California Santa Cruz"
    email: "`[ivo|carlosm]@cs.ucsc.edu`"
  - name: "Adam Moody, Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "`[moody11|kathryn]@llnl.gov`"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "`gflofst@sandia.gov`"
  - name: "Remzi Arpaci-Dusseau, Andrea Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "`[remzi|dusseau]@cs.wisc.edu`"
abstract: |
  Evaluating experimental results in the field of computer systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we analyze salient features of container technology 
  that, if leveraged correctly, can help reduce the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of distributed storage systems to illustrate the 
  extensions that we envision, mainly in terms of container management 
  infrastructure. We also discuss the benefits and limitations of 
  using containers as a way of reproducing research in other areas of 
  experimental systems research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
links-as-notes: true
substitute-hyperref: true
documentclass: ieeetran
classoption: conference
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous experiments. Registering information about an 
experiment allows scientists to interpret and understand results, as 
well as verify that the experiment was performed according to 
acceptable procedures. Additionally, reproducibility plays a major 
role in education since the amount of information that a student has 
to digest increases as the pace of scientific discovery accelerates. 
By having the ability to repeat experiments, a student can learn by 
looking at provenance information, re-evaluate the questions that the 
original experiment answered and thus "stand on the shoulder of 
giants".

In applied computer science an experiment is carried out in its 
entirety on a computer. Repeating an experiment doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`); however, this is 
not the case. Today's computational environments are complex and 
accounting for all possible effects of changes within and across 
systems is a challenging task [@freire_computational_2012 ; 
@donoho_reproducible_2009].

Version control systems (VCS) are sometimes used to address some of 
these problems. By having a particular version ID for the software 
used for an article's experimental results, reviewers and readers can 
have access to the same code base [@brown_how_2014]. However, 
availability of the source code does not guarantee reproducibility 
[@collberg_measuring_2014] since the code might not compile and, even 
if compilable, the results might differ. In that case, the differences 
have to be analyzed in order to corroborate the validity of the 
original experiment.

Additionally, reproducing experimental results when the underlying 
hardware environment changes is challenging mainly due to the 
inability to predict the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads in terms of performance (the hypervisor 
"tax") and management (creating, storing and transferring) can be high 
and, in some fields of computer science such as systems research, 
cannot be accounted for easily [@clark_xen_2004].

OS-level virtualization [@soltesz_container-based_2007] is a server 
virtualization method where the kernel of an operating system allows 
for multiple isolated user space instances, instead of just one. Such 
instances (often called containers, virtualization engines (VE), 
virtual private servers (VPS), or jails) may look and feel like a real 
server from the point of view of its owners and users. In addition to 
isolation mechanisms, the kernel often provides resource management 
features to limit the impact of one container's activities on the 
other containers. Container technology is currently employed as a way 
of reducing the complexity of software deployment and portability of 
applications in cloud computing infrastructure. Arguably, containers 
have taken the role that package management tools had in the past, 
where they were used to control upgrades and keep track of change in 
the dependencies of an application [@di_cosmo_package_2008].

In this work, we make the case for containers as a way of tackling 
some of the reproducibility problems in computer systems research. 
Specifically, we propose to use the resource accounting and limiting 
components of OS-level virtualization as a basis for creating 
execution profiles of experiments that can be associated with results, 
so that these can subsequently be analyzed when an experiment is 
evaluated. In order to reduce the problem space, we focus on local and 
distributed storage systems in various forms (e.g., local file 
systems, distributed file systems, key-value stores, and related 
data-storage engines) since this is one of the most important areas 
underlying cloud computing and big-data processing, as well as our 
area of expertise.

The rest of this paper is organized as follows. We first describe the 
distinct levels of reproducibility that can be associated with 
scientific claims in systems research and give concrete examples in 
the area of storage [systems](#experiment). We then analyze salient 
features of container technology that are relevant in the evaluation 
of experiments and introduce what in our view is missing in order to 
make containers a useful reproducibility tool for storage systems 
[research](#extension). We subsequently present a use case that 
illustrates the benefits of the proposed container management 
extensions that we [envision](#case). We then follow with a discussion 
about the benefits and limitations of using containers in other areas 
of experimental systems [research](#discussion). We finally discuss 
related [work](#related) and [conclude](#conclusion).

# Evaluation of Experimental Systems Research {#experiment}

An experiment in systems research is composed of a triplet of \(1\) 
workload, (2) a specific system where the workload runs and \(3\) 
results from a particular execution. Respective to this order is the 
complexity associated with the evaluation of an experiment: obtaining 
the exact same results is more difficult than just getting access to 
the original workload. Thus, we can define a taxonomy to characterize 
the reproducibility of experiments:

 1. _Workload Reproducibility_. We have access to the original code 
    and the particular workload that was used to obtain the original 
    experimental results.
 2. _System Reproducibility_. We have access to hardware and software 
    resources that resemble the original dependencies.
 3. _Results Reproducibility_. The results of the re-execution of an 
    experiment are valid with respect to the original.

In storage systems research, workload reproducibility is achieved by 
getting access to the configuration of the benchmarking tool that 
defines the I/O patterns of the experiment. For example, if an 
experiment uses the [Flexible I/O Tester][fio] (FIO), then the 
workload is defined by the FIO input file.

System reproducibility can be divided into software and hardware. The 
former corresponds to the entire software stack from the 
firmware/kernel up to the libraries used by an experiment. The latter 
comprises the set of hardware devices involved in the experiment such 
as the specific CPU model, storage drives or network cards on which 
the experiment ran.

Reproducing results does not necessarily imply the regeneration of the 
exact same measurements; instead it entails validating the results by 
checking how close (in shape or trends) to the original experiment 
they are. Given this, evaluating an experiment can be a subjective 
task. We propose metrics within the domain of storage systems in terms 
of resource utilization, specifically memory, CPU, and bandwidth, as a 
to provide objective standards for unambiguously comparing results.

# Containers For Reproducible Systems Research {#extension}

Current implementations of OS-level virtualization (e.g. [LXC]  or 
[OpenVZ]) include an accounting component that keeps track of the 
resource utilization of a container over time. In general, this module 
can account for CPU, memory, network and I/O usage. By periodically 
checking and recording these metrics while an experiment runs, we can 
obtain a profile of its execution. This profile is the signature of 
the experiment on the particular hardware on which it executed. The 
challenge is to recreate results on distinct hardware. By coupling 
this execution profile to a profile of the underlying hardware, we can 
provide valuable information for researchers to use while evaluating a 
particular result. In concrete, when trying to reproduce an experiment 
on a system $B$ that originally ran on $A$, we propose the following 
mapping methodology:

  1. Obtain the hardware profile of $A$.
  2. Obtain the configuration of every container involved in the 
     experiment, along with their execution profile.
  3. Obtain the hardware profile of $B$.
  4. Generate a configuration for the experiment w.r.t. $B$ by 
     recreating the resource allocation that the experiment had when 
     it ran on $A$.

The hardware profile is composed of static (e.g. the hardware 
characteristics as seen by `lshw`) and dynamic information (e.g. the 
execution of micro-benchmarks to characterize the bare-metal 
performance of a machine). The container configuration corresponds to 
the resources available to a container, such as number of CPUs and 
total amount of memory. While the experiment runs, performance metrics 
for the container can be obtained in order to create an execution 
profile.

Using LXC as an example, we show in Figure 1 a monitoring daemon 
running as a userspace process in the host. This process periodically 
dumps the content of the `cgroups` pseudo-filesystem in order to 
capture the runtime metrics of containers running in the system.

![A userpace process running alongside the container execution engine 
(LXC) that periodically probes the statistics of containers in order 
to obtain an execution profile.](/root/figures/monitor.png)

An alternative for structuring this information is by defining the 
following schema:

`(IMG | EX_ID | HW_PROFILE | CGROUPS | EX_PROFILE)`

Where `IMG` points to the image from where the container was 
instantiated. `EXE_ID` corresponds to a particular execution of the 
experiment with associated timestamps. `HW_PROFILE`, as mentioned 
above, captures the bare-metal performance of the machine where the 
container executes. `CGROUPS` is the configuration in terms of control 
groups for the distinct subsystems (CPU, memory, network and I/O). 
`EX_PROFILE` is the profile for a particular execution; i.e. there is 
one profile for every entry (one for each `EX_ID`).

The profile database can be located remotely in a central repository 
that serves as the hub for managing experiments in a distributed 
environment. For example, this monitoring component could be 
implemented as a submodule of CloudLab [@ricci_introducing_2014]. For 
experiments consisting of multiple hosts and container images, 
orchestration tools such as Mesos [@hindman_mesos_2011] can also be 
extended to incorporate this profiling functionality. One of main 
goals of our work is to determine whether this information is 
sufficient to evaluate a result.

# Use Case: Scalability Experiments of Ceph OSDI '06 {#case}

To illustrate the utility of having execution and hardware profiles, 
we take the Ceph OSDI '06 paper [@weil_ceph_2006] and reproduce one of 
its experiments. In particular, we look at the scalability experiment 
from the data performance section (6.1). The reason for selecting this 
paper is (1) our familiarity with these experiments makes it easier to 
reason about contextual information not necessarily available directly 
from the paper and (2) the original hardware is still available in our 
laboratory. Each node in the system consist of a 2-core 2212 AMD 
Opteron @2.0GHz, 8GB of RAM, 1GbE NIC and 250GB Seagate Barracuda ES 
hard drives. We created a containerized version of the experiment 
using the 0.87 branch of Ceph. We use docker 1.3.3 and LXC 1.0.6 
running on Ubuntu 12.04 hosts (3.13.0-43 x86_64 kernel).

The experiments in Section 6.1 of the original paper showed the 
ability of Ceph to saturate disk evenly among the drives of the 
cluster. Figures 5-7 from the original paper showed per-OSD 
performance as the object size varied from 4 KB to 4 MB. In our case 
we focus on reproducing the scalability experiment of Section 6.1.3 
(Figure 8), which used 4 MB objects, to minimize random I/O noise from 
the hard drives. We ignore the performance of the `hash` data 
distribution and fix the number of placement groups to 32K, thus we 
meaningfully compare against the red solid-dotted line in Figure 8.

The original scalability experiment ran with 20 clients per node on 20 
nodes (400 clients total) and varied the number of OSDs from 2-26 in 
increments of 2. Every node was connected via 1 GbE link, so the 
experiment theoretical upper bound was 2GB/s (when there was enough 
capacity of the OSD cluster to have 20 1Gb connections) or 
alternatively when the connection limit of the switch was reached. The 
paper experiments were executed on a Netgear switch. This device has a 
capacity of approximately 14 GbE in _real_ total traffic (from a 20 
advertised), corresponding to the 24 * 58 = 1400 MB/s combined 
throughput shown in the original paper.

![Reproducing a scaled-down version of the original OSDI '06 
scalability experiment (Figure 8). The y-axis represents average 
throughput, as seen by the client. They x-axis corresponds to the size 
of the cluster (in number of object storage devices (OSD). The square 
marker corresponds to the average of 10 executions of the experiment. 
It also represents the overall performance of the hard disks in the 
cluster. The line with triangle markers projects the original results 
to our setting.](/root/figures/ceph1.png)

We scaled down the experiment by reducing the number of clients nodes 
to 1 (16 clients). This means that our network upper bound is 110 MB/s 
(the capacity of the 1GbE link from the client to the switch). We 
throttle I/O at 30 MB/s (vs. 58 MB/s of the original paper), so this 
is our scaling unit (per-OSD increment). The reason for throttling at 
30 MB/s is that, over time, the Seagate disks have aged (they are 10 
years old!) and overall performance among the hard drives of our 
cluster is different from the ~58 MB/s observed in the original paper. 
In order to amortize, we had to take the lowest common denominator 
which in this case is 30 MB/s. We throttle I/O by configuring Ceph 
containers with the control group `blkio.throttle.write_bps_device` 
directive. Figure 2 shows results of this scaled-down, throttled 
version of the scalability experiment. An open question is to 
determine if the process of scaling-down and throttling resources can 
be automated, given the profile repository described in the previous 
section.

We see that Ceph scales linearly with the number of OSDs, up to the 
point where we saturate the 1GbE link. We note that we don't see 30 
MB/s of net I/O utilization since the current version of Ceph issues 
two I/O calls on each write request, one to the write-ahead log and 
another one to the data backend. The original experiments used a 
prototype version of Ceph that didn't include this 
atomicity/durability feature. Figure 2 also shows a projection of the 
original data to our setting. The original experiment show better 
scalability behavior due to newer and more stable hardware.

## Reproducing Results on Different Hardware

So far we have discussed how to reproduce an experiment on the 
original hardware. But, as we have mentioned before, the challenge is 
in reproducing experiments on different hardware. Having the 
experiment implemented in containers allows us to swap components of 
the underlying hardware and repeat the experiment easily; after all, 
this is one of the ultimate goals of virtualization, and containers 
aim at doing it with minimal overhead. Our interest is in measuring 
the effects that replacing distinct components has on experimental 
results. Our conjecture is that, for many cases, the methodology 
defined in the previous section will allow to reproduce results on 
distinct hardware. As part of our efforts, we are working in 
characterizing the cases for which our methodology will work and those 
for which it won't.

![Showing the effect of replacing 4 hard drives with newer models. Old 
hard drives are the same used in the previous figure and correspond to 
a set of 10 year old 250GB Seagate Barracuda ES (ST3250620NS). New 
hard drives correspond to 500GB Western Digital Re (WD5003ABYZ) 
drives.](/root/figures/ceph2.png)

Thus, one of our initial goals is to empirically test the 
repercussions of replacing network, CPU, storage and memory devices 
(among others). We now present preliminary results on the outcome of 
swapping distinct storage drives. We re-executed the scalability 
experiment, swapping four old hard drives with newer models. The 
results are shown in Figure 3.

The reason for the change in results with respect to the first four 
points of Figure 2 is the following. As mentioned earlier, Ceph issues 
two I/O calls on each write request, one of them being asynchronous. 
The cgroups `blkio` controller responsible for limiting I/O on block 
devices (which we configure to 30MB/s) cannot throttle asynchronous 
I/O operations since this type of requests go to a queue that is 
shared at the system level by all containers running in the host. We 
experientially corroborated this by executing a microbenchmark using 
FIO that executed the same load (4MB files) on the two hard drives in 
question, but using direct I/O exclusively. In this case, the 
performance corresponds to the throttled 30 MB/s. We then executed a 
mixed workload of both direct and async I/O requests and observed that 
the newer hard drive performs better than the old one, with similar 
results as those showed in Figure 3.

# Discussion {#discussion}

We discuss other benefits and limitations of containerization, as well 
as general reproducibility guidelines when working with containers.

## Cataloging Experiments

By storing performance profiles of experiments and associated 
hardware, we can create categories of container metrics that describe 
in a high-level what the experiment's goal is, for example:

  * In-memory only
  * Storage intensive
  * Network intensive
  * CPU intensive
  * 50% of caching effects

Assuming there is a central repository of experiments such as CloudLab 
[@ricci_introducing_2014]. A scientist wanting to test new ideas in 
systems research can look for experiments in this database by issuing 
queries with a particular category in mind.

## Can All Systems Research Be Containerized? {#all}

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * Distinct experiments consolidated on the same host.
  * For an experiment, containers with conflicting roles need to be 
    co-located in the same physical host (e.g. two database instances 
    on the same host).
  * Kernel version can't be frozen.

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects. This list is not comprehensive, of course, and an open 
problem is to delineate precisely the boundaries between experiments 
well-suited to be reproducible via containers vs. those that are not.

## Other Lessons Learned So Far

We list some of the lessons that we have learned as part of our 
experience in implementing experiments in containers:

  * Version control the experiment's code and its dependencies, 
    leveraging git subtrees/submodules (or alike) to keep track of 
    inter-dependencies between projects. For example, if a git 
    repository contains the definition of a Dockerfile, make it a 
    submodule of the main project.
  * Refer to the specific version ID that a paper's results were 
    obtained from. Git's tagging feature can also be used to point to 
    the version that contains the codebase for an experiment (e.g. 
    "sosp14").
  * When possible, add experimental results as part of the commit that 
    contains the codebase of an experiment. In other words, try to 
    make the experiment as self-contained as possible, so that 
    checking out that particular version contains all the dependencies 
    and generated data.
  * Keep a downloadable container image for the version of the 
    experiment codebase (e.g. use the docker registry and its 
    automated build feature).
  * Whenever possible, use CI technologies to ensure that changes to 
    the codebase don't disrupt the reproducibility of the experiment.
  * Obtain a profile of the hardware used (eg. making use of tools 
    such as [SoSReport]), as well as the resource configuration of 
    every container and publish these as part of the experimental 
    results (i.e. add it to the commit that a paper's results are 
    based on).

We followed this guidelines for this paper. All the resources needed 
to generate its content can be found in our github 
[account](https://github.com/ivotron/woc_2015#camera-ready).

# Related Work {#related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized 
[@ignizio_establishment_1971]. This issue has recently received a 
significant amount of attention from the computational research 
community [@freire_computational_2012 ; @neylon_changing_2012 ; 
@leveqije_reproducible_2012], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave runtime 
performance as a secondary problem. In systems research, runtime 
performance _is_ the subject of study, thus we need to look at it as a 
primary issue. By obtaining profiles of executions and making them 
part of the results, we allow researchers to validate experiments with 
performance in mind.

In [@collberg_measuring_2014], the authors took 613 articles published 
in 13 top-tier systems research conferences and found that 25% of the 
articles are reproducible (under their reproducibility criteria). The 
authors did not analyze performance. In our case, we are interested 
not only in being able to rebuild binaries and run them but also in 
evaluating the performance characteristics of the results.

Containers, and specifically docker, have been the subject of recent 
efforts that try to alleviate some of the reproducibility problems in 
data science [@boettiger_introduction_2014]. Existing tools such as 
Reprozip [@chirigati_reprozip_2013] package an experiment in a 
container without having to initially implement it in one (i.e. 
automates the creation of a container from an "non-containerized" 
environment). Our work is complementary in the sense that we look at 
the conditions in which the experiment can be validated in terms of 
performance behavior if it runs within a container.

# Conclusion {#conclusion}

In this paper we have presented our proposal for complementing 
container management infrastructure to capture execution profiles with 
the purpose of making these available to experimental research 
reviewers and readers. We illustrated the benefits of our hardware and 
container execution profiling methodology by reproducing a previously 
published experiment in the area of distributed storage systems. We 
are currently in the process of further testing these ideas on 
experiments from other areas of systems research such as kernel 
development and network systems.

<!--
**Acknowledgements:** This work was partially supported by NSF grant 
... We wish to thank ...
-->

# References

[SoSReport]: https://www.github.com/sosreport/sos
[fio]: https://github.com/axboe/fio
[LXC]: https://www.kernel.org/doc/Documentation/cgroups
[OpenVZ]: https://wiki.openvz.org/Proc/user_beancounters

<!-- hanged biblio -->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}


