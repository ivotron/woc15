---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez"
    affiliation: "UC Santa Cruz"
    email: "ivo@cs.ucsc.edu"
  - name: "Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "carlosm@ucsc.edu"
  - name: "Adam Moody"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "moody11@llnl.gov"
  - name: "Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "kathryn@llnl.gov"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "gflofst@sandia.gov"
  - name: "Remzi Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "remzi@cs.wisc.edu"
abstract: |
  Evaluating experimental results in the field of Computer Systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper we analyze salient features of container technology 
  that, if leveraged correctly, can help in reducing the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of Storage Systems to illustrate the extensions that are 
  required, mainly in terms of container management infrastructure. We 
  also discuss the benefits and limitations of using containers as a 
  way of reproducing research in other areas of experimental systems 
  research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
documentclass: acm-sig-alternate
substitute-hyperref: true
disable-copyright: true
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous results. Registering information about an 
experiment (referred to as _provenance_) such as annotations, diagrams 
and lists, allows scientists to interpret and understand results. By 
examining the steps that led to a result, scientists can gain insights 
into the chain of reasoning used in its production, verify that the 
experiment was performed according to acceptable procedures, identify 
the experiment's inputs, and, in some cases, regenerate the original 
result [@freire_provenance_2008]. Additionally, reproducibility plays 
a major roll in education since the amount of information that a 
student has to digest increases as the pace of scientific discovery 
accelerates. By having repeatable experiments, a student can learn by 
looking at provenance information, re-evaluate the questions that the 
original experiment answered and thus "stand in the shoulder of 
giants".

In applied Computer Science an experiment is carried out in its 
entirety by a computer [@freire_computational_2012 ; 
@donoho_reproducible_2009]. Although the scientist is interactively 
defining the sequence of steps that get executed, ultimately binary 
programs run on a machine. Repeating a result doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`). With our current 
computational abilities to collect and manage data we should be able 
to record and reconstruct environments for particular results 
perfectly. However, this is not the case, mainly due to three factors:

  1. Association between experiment and its codebase.
  2. Evolving software.
  3. Changes in hardware.

Version control systems (VCS) are sometimes used to tackle 1 and 2. By 
having a particular version ID for the software used for an article's 
experimental results, reviewers and readers can have access to the 
same codebase [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_measuring_2014] 
since the code might not compile and, even if compilable, the results 
might differ, in which case the differences have to be analyzed in 
order to corroborate the validity of the original experiment.

Reproducing experimental results when the underlying hardware 
environment changes (3) is challenging mainly due to the inability of 
predicting the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads associated in terms of performance (the 
hypervisor's "tax") and management (creating, storing and transferring 
them) can be high and, in some fields of Computer Science such as 
Computer Systems research, cannot be accounted easily 
[@clark_xen_2004].

Container technology [@soltesz_container-based_2007] is currently 
employed as a way of reducing the complexity of software deployment in 
cloud computing infrastructure and has taken the role that package 
management tools had in the past, where they were used to control 
upgrades and keep track of change in the dependencies of an 
application [@dicosmo_package_2008]. In this work, we make the case 
for containers as a way of tackling some of the reproducibility 
problems in Computer Systems research. In order to reduce the problem 
space, we focus to local and distributed storage systems in various 
forms (e.g., local file systems, distributed file systems, key-value 
stores, and related data-storage engines) since this is one of the 
most important areas underlying cloud computing and big-data 
processing, as well as our area of expertise.

We first describe the salient features of container technology that 
are relevant in the evaluation of experiments. We then describe what 
in our view is missing in order to make containers a useful 
reproducibility tool for storage systems research. We then present a 
use case that illustrates the container management extensions that we 
envision that would allow to record metadata about the execution of 
experiments. We then describe how this metadata can be used to 
evaluate an experiment and, at the same time, create a catalog that 
researches can use when looking for particular types of experiments. 
We then discuss the benefits and limitations of using containers as a 
way of reproducing research in other areas of experimental systems 
research. We conclude and delineate future work.

# Containers for

Containers offer a set of features that, In order for containers to be 
considered an alternative.

--------------

  * Helps reproducing functionality in a particular environment.
  * Docker is package management
  * Containers have been very useful for this


  * When we want to reproduce an experiment.

Define Experiment: a particular workload, on particular system with 
particular results

  * A workload on a tested system and
  * System
  * Results

So we have to talk about what's missing in terms of container 
management.

  * we want to extend what a container has, in terms of statistics


-----------

today reproducibility is focusing on deployment

we need workloads, platforms => particular resource allocations


containers have the potential of cover all of these:


but we need more


--------------


## Contributions

In order for container technologies to be a viable solution to the 
reproducibility problem in storage systems research, we need to 
address the following:

  1. What characteristics does an experiment must observe in order to 
     not be affected by the overhead associated to containers 
     [technologies](#characteristics)?
  2. How can an experiment be reproduced in infrastructure that 
     differs from where it was originally [executed](#mapping)?

# Experiments Suitable for Containers {#characteristics}

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * More than one container needs to be co-located in the same 
    physical host
  * Kernel version can't be frozen

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects.

----------

**TODO**: do we need to execute experiments of our own?

----------

# Evaluation on Different Hardware {#mapping}

For experiments suitable to be containerized, we define a mapping 
methodology that allows to evaluate experimental results on hardware 
distinct than the original:

  1. Obtain the profile of the original hardware
  2. Obtain the configuration of each container (e.g. User Beacons or 
     cgroup configuration)
  3. Obtain the profile of the new hardware
  4. Generate a configuration for the new hardware based on 2-4

----------

**TODO**: use ceph use case (below) or pick another one like [fio]. In 
general, we show that our mapping methodology works.

----------

## Use Case

We look at the Ceph OSDI '06 paper as an example of the difficulties 
in trying to reproduce Systems Research.

\ 

**TODO**: should we have this section? Should it appear before the 
_Challenges_ section? Also, we should determine if we want to include 
the table; explain what we look at so that we can talk about the 
table.

\ 

We then present the table and explain that so far we have looked at 
show 5 and 13.

```
| Setup | HW | VM | Kernel | Code | Time | Outcome |
|:-----:|:--:|:--:|:-----:|:----:|:----:|:------:|
|  01   |old | on |  new   | new  |      |         |
|  02   |old | on |  new   | old  |      |         |
|  03   |old | on |  old   | new  |      |         |
|  04   |old | on |  old   | old  |      |         |
|  05   |old | off|  new   | new  |  90  |   RP    |
|  06   |old | off|  new   | old  |      |         |
|  07   |old | off|  old   | old  |      |         |
|  08   |old | off|  old   | new  |      |         |
|  09   |new | on |  new   | new  |      |         |
|  10   |new | on |  new   | old  |      |         |
|  11   |new | on |  old   | old  |      |         |
|  12   |new | on |  old   | new  |      |         |
|  13   |new | off|  new   | new  |      |         |
|  14   |new | off|  new   | old  |      |         |
|  15   |new | off|  old   | old  |      |         |
|  16   |new | off|  old   | new  |      |         |
```

The `Outcome` column corresponds to the outcome of the experiment. 
Possible values are:

  * _RT_. Repeated the experiments with exactly the same numbers.
  * _RP_. Reproduced experiments by validating the original results, 
    in the sense that the same conclusion about the original work can 
    be made. In other words, the experiment is validated
  * NB. Unable to build

The time column corresponds to the time it took to achieve the 
reproducibility level specified.

# Lessons Learned So Far

  1. Version control everything and use git subtrees or submodules to 
     keep track of inter-dependencies between projects. For example, 
     if a repository contains the definition of a container, make it a 
     submodule of the main project.
  2. Obtain a profile of the hardware used (eg. making use of tools 
     such as SoSReport[^sosreport]) and publish it as part of the 
     experimental results.
  3. Keep a container image for every runnable version of the 
     experiment codebase.
  4. Whenever possible, use CI technologies to ensure that changes to 
     the experiment's codebase don't disrupt its reproducibility (as 
     defined in the introduction).

# Related Work

  * Recent efforts such as _The Recomputation Manifesto_ [^recomp] 
    leave runtime performance as a secondary problem. For most of the 
    work in Systems Research, runtime performance _is_ the subject of 
    research thus we need to look at it as a primary issue.

  * How we differ/extend the work in [@collberg_measuring_2014]? 
    **Answer**: we are interested not only in being able to rebuild 
    binaries but also evaluate the validity of the results.

  * Existing tools such as Reprozip [@ ] try to package an experiment 
    in a container without having to implement it in one (i.e. 
    automates the creation of a container from an "uncontainerized" 
    environment). Our work is complementary in the sense that we look 
    at the conditions in which the experiment can be validated in 
    order tu ensure that the containerized version will (or won't) 
    produce valid results.

[^recomp]: www.recomputation.org/manifesto/

# Future Work

Take papers from other sub-disciplines of Systems Research:

  * Kernel
  * Network
  * Hardware

How can we repeat the effort presented in this paper for the above?

# References

[^sosreport]: https://github.com/sosreport/sos

[fio]: https://github.com/axboe/fio/

<!-- hanged biblio-->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
