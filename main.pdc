---
title: "Container Technology and Its Role in Reproducible Storage Systems Research"

The Role of Container in Reproducible Systems Evaluation

author:
  - name: "Ivo Jimenez"
    affiliation: "UC Santa Cruz"
    email: "ivo@cs.ucsc.edu"
  - name: "Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "carlosm@ucsc.edu"
  - name: "Adam Moody"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "moody11@llnl.gov"
  - name: "Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "kathryn@llnl.gov"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "gflofst@sandia.gov"
  - name: "Remzi Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "gflofst@sandia.gov"
abstract: |
  Container technology promises to be of good use for solving 
  reproducibility challenges in computer science. For the particular 
  case of Storage Systems research, there is a set of features that 
  containers lack
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
documentclass: acm-sig-alternate
substitute-hyperref: true
disable-copyright: true
---

This is not for the paper:

--------

over time disk have aged and performance profile is different. In 
order to get uniform performance, we had to throttle (because old 
sectors, etc..). In order to amortize, we had to take the lowest 
common denominator. This allowed us to reproduce without on old 
hardware. This works on the new 

In order to reproduce, we have to mess with them in some.

--------

overleaf

<carlosm@ucsc.edu>

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous results. Registering information about an 
experiment (referred to as _provenance_) such as annotations, diagrams 
and lists, allows scientists to interpret and understand results. By 
examining the steps that led to a result, scientists can gain insights 
into the chain of reasoning used in its production, verify that the 
experiment was performed according to acceptable procedures, identify 
the experiment's inputs, and, in some cases, regenerate the original 
result. Additionally, reproducibility plays a major roll in education 
since the amount of information that a student has to digest increases 
as the pace of scientific discovery accelerates. By having repeatable 
experiments, a student can learn by looking at provenance information, 
re-evaluate the questions that the original experiment answered and 
thus "stand in the shoulder of giants".

In applied Computer Science an experiment is carried out in its 
entirety by a computer. Although the scientist is interactively 
defining the sequence of steps that get executed, ultimately binary 
programs run on a machine. Repeating a result doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`). With our current 
computational abilities to collect and manage data we should be able 
to record and reconstruct environments for particular results 
perfectly. However, this is not the case, mainly due to three factors:

  1. Lack of association between an experiment and its codebase.
  2. Evolving software.
  3. Changes in hardware.

Version control systems (VCS) are sometimes used to tackle 1 and 2. By 
having a particular version ID for the software used for an article's 
experimental results, reviewers and readers can access to the same 
codebase [@brown_how_2014]. However, access to source code is not a 
guarantee for reproducing an experiment [@collberg_measuring_2014] 
since it might not be possible to compile it and, even if compilable, 
the results might differ, in which case the new computational 
environment has to be evaluated in order to corroborate the validity 
of the original experiment.

Reproducing experimental results when the underlying hardware 
environment changes is challenging because the effect of such changes 
in the outcome of an experiment cannot be predicted accurately. A 
Virtual Machine (VM) can be used to partially address this issue but 
the overheads of creating, storing and transferring the derived images 
can be high, which in some fields of computer science cannot be 
neglected [@clark_xen_2004]. Container technology 
[@soltesz_container-based_2007] can be

In order to reduce the problem space, we focus in Systems Research in 
general and Storage in particular.

--------------

  * Helps reproducing functionality in a particular environment.
  * Docker is package management
  * Containers have been very useful for this


  * When we want to reproduce an experiment.

Define Experiment: a particular workload, on particular system with 
particular results

  * A workload on a tested system and
  * System
  * Results

So we have to talk about what's missing in terms of container 
management.

  * we want to extend what a container has, in terms of statistics


-----------

today reproducibility is focusing on deployment

we need workloads, platforms => particular resource allocations


containers have the potential of cover all of these:


but we need more


--------------


## Contributions

In order for container technologies to be a viable solution to the 
reproducibility problem in storage systems research, we need to 
address the following:

  1. What characteristics does an experiment must observe in order to 
     not be affected by the overhead associated to containers 
     [technologies](#characteristics)?
  2. How can an experiment be reproduced in infrastructure that 
     differs from where it was originally [executed](#mapping)?

# Experiments Suitable for Containers {#characteristics}

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * More than one container needs to be co-located in the same 
    physical host
  * Kernel version can't be frozen

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects.

----------

**TODO**: do we need to execute experiments of our own?

----------

# Evaluation on Different Hardware {#mapping}

For experiments suitable to be containerized, we define a mapping 
methodology that allows to evaluate experimental results on hardware 
distinct than the original:

  1. Obtain the profile of the original hardware
  2. Obtain the configuration of each container (e.g. User Beacons or 
     cgroup configuration)
  3. Obtain the profile of the new hardware
  4. Generate a configuration for the new hardware based on 2-4

----------

**TODO**: use ceph use case (below) or pick another one like [fio]. In 
general, we show that our mapping methodology works.

----------

## Use Case

We look at the Ceph OSDI '06 paper as an example of the difficulties 
in trying to reproduce Systems Research.

\ 

**TODO**: should we have this section? Should it appear before the 
_Challenges_ section? Also, we should determine if we want to include 
the table; explain what we look at so that we can talk about the 
table.

\ 

We then present the table and explain that so far we have looked at 
show 5 and 13.

```
| Setup | HW | VM | Kernel | Code | Time | Outcome |
|:-----:|:--:|:--:|:-----:|:----:|:----:|:------:|
|  01   |old | on |  new   | new  |      |         |
|  02   |old | on |  new   | old  |      |         |
|  03   |old | on |  old   | new  |      |         |
|  04   |old | on |  old   | old  |      |         |
|  05   |old | off|  new   | new  |  90  |   RP    |
|  06   |old | off|  new   | old  |      |         |
|  07   |old | off|  old   | old  |      |         |
|  08   |old | off|  old   | new  |      |         |
|  09   |new | on |  new   | new  |      |         |
|  10   |new | on |  new   | old  |      |         |
|  11   |new | on |  old   | old  |      |         |
|  12   |new | on |  old   | new  |      |         |
|  13   |new | off|  new   | new  |      |         |
|  14   |new | off|  new   | old  |      |         |
|  15   |new | off|  old   | old  |      |         |
|  16   |new | off|  old   | new  |      |         |
```

The `Outcome` column corresponds to the outcome of the experiment. 
Possible values are:

  * _RT_. Repeated the experiments with exactly the same numbers.
  * _RP_. Reproduced experiments by validating the original results, 
    in the sense that the same conclusion about the original work can 
    be made. In other words, the experiment is validated
  * NB. Unable to build

The time column corresponds to the time it took to achieve the 
reproducibility level specified.

# Lessons Learned So Far

  1. Version control everything and use git subtrees or submodules to 
     keep track of inter-dependencies between projects. For example, 
     if a repository contains the definition of a container, make it a 
     submodule of the main project.
  2. Obtain a profile of the hardware used (eg. making use of tools 
     such as SoSReport[^sosreport]) and publish it as part of the 
     experimental results.
  3. Keep a container image for every runnable version of the 
     experiment codebase.
  4. Whenever possible, use CI technologies to ensure that changes to 
     the experiment's codebase don't disrupt its reproducibility (as 
     defined in the introduction).

# Related Work

  * Recent efforts such as _The Recomputation Manifesto_ [^recomp] 
    leave runtime performance as a secondary problem. For most of the 
    work in Systems Research, runtime performance _is_ the subject of 
    research thus we need to look at it as a primary issue.

  * How we differ/extend the work in [@collberg_measuring_2014]? 
    **Answer**: we are interested not only in being able to rebuild 
    binaries but also evaluate the validity of the results.

  * Existing tools such as Reprozip [@ ] try to package an experiment 
    in a container without having to implement it in one (i.e. 
    automates the creation of a container from an "uncontainerized" 
    environment). Our work is complementary in the sense that we look 
    at the conditions in which the experiment can be validated in 
    order tu ensure that the containerized version will (or won't) 
    produce valid results.

[^recomp]: www.recomputation.org/manifesto/

# Future Work

Take papers from other sub-disciplines of Systems Research:

  * Kernel
  * Network
  * Hardware

How can we repeat the effort presented in this paper for the above?

# References

[^sosreport]: https://github.com/sosreport/sos

[fio]: https://github.com/axboe/fio/

<!-- hanged biblio-->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
