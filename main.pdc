---
title: "The Role of Container Technology in Reproducible Computer Systems Research"
author:
  - name: "Ivo Jimenez"
    affiliation: "UC Santa Cruz"
    email: "ivo@cs.ucsc.edu"
  - name: "Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "carlosm@ucsc.edu"
  - name: "Adam Moody"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "moody11@llnl.gov"
  - name: "Kathryn Mohror"
    affiliation: "Lawrence Livermore National Laboratories"
    email: "kathryn@llnl.gov"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Laboratories"
    email: "gflofst@sandia.gov"
  - name: "Remzi Arpaci-Dusseau"
    affiliation: "University of Wisconsin-Madison"
    email: "remzi@cs.wisc.edu"
abstract: |
  Evaluating experimental results in the field of Computer Systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we analyze salient features of container technology 
  that, if leveraged correctly, can help reducing the complexity of 
  reproducing experiments in systems research. We present a use case 
  in the area of storage systems to illustrate the extensions that we 
  envision, mainly in terms of container management infrastructure. We 
  also discuss the benefits and limitations of using containers as a 
  way of reproducing research in other areas of experimental systems 
  research.
tags:
  - phdthesis
  - workshop-paper
  - woc15
category: labnotebook
layout: paper
numbersections: true
substitute-hyperref: true
documentclass: acm-sig-alternate
disable-copyright: true
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous results. Registering information about an 
experiment (referred to as _provenance_) such as annotations, diagrams 
and lists, allows scientists to interpret and understand results. By 
examining the steps that led to a result, scientists can gain insights 
into the chain of reasoning used in its production, verify that the 
experiment was performed according to acceptable procedures, identify 
the experiment's inputs, and, in some cases, regenerate the original 
result [@freire_provenance_2008]. Additionally, reproducibility plays 
a major roll in education since the amount of information that a 
student has to digest increases as the pace of scientific discovery 
accelerates. By having repeatable experiments, a student can learn by 
looking at provenance information, re-evaluate the questions that the 
original experiment answered and thus "stand in the shoulder of 
giants".

In applied Computer Science an experiment is carried out in its 
entirety by a computer [@freire_computational_2012 ; 
@donoho_reproducible_2009]. Although the scientist is interactively 
defining the sequence of steps that get executed, ultimately binary 
programs run on a machine. Repeating a result doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`). With our current 
computational abilities to collect and manage data we should be able 
to record and reconstruct environments for particular results 
perfectly. However, this is not the case, mainly due to three factors:

  1. Availability of an experiment's dependencies[^andalso].
  2. Evolving software.
  3. Changes in hardware.

Version control systems (VCS) are sometimes used to tackle 1 and 2. By 
having a particular version ID for the software used for an article's 
experimental results, reviewers and readers can have access to the 
same codebase [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_measuring_2014] 
since the code might not compile and, even if compilable, the results 
might differ, in which case the differences have to be analyzed in 
order to corroborate the validity of the original experiment.

[^andalso]: By dependencies we mean the codebase and all other assets 
associated to results, such as input/output files and library 
dependencies.

Reproducing experimental results when the underlying hardware 
environment changes (3) is challenging mainly due to the inability of 
predicting the effects of such changes in the outcome of an 
experiment. A Virtual Machine (VM) can be used to partially address 
this issue but the overheads associated in terms of performance (the 
hypervisor's "tax") and management (creating, storing and transferring 
them) can be high and, in some fields of Computer Science such as 
Computer Systems research, cannot be accounted easily 
[@clark_xen_2004].

Container technology [@soltesz_container-based_2007] is currently 
employed as a way of reducing the complexity of software deployment 
and portability of applications in cloud computing infrastructure. 
Arguably, containers have taken the role that package management tools 
had in the past, where they were used to control upgrades and keep 
track of change in the dependencies of an application 
[@di_cosmo_package_2008]. In this work, we make the case for 
containers as a way of tackling some of the reproducibility problems 
in Computer Systems research. In order to reduce the problem space, we 
focus to local and distributed storage systems in various forms (e.g., 
local file systems, distributed file systems, key-value stores, and 
related data-storage engines) since this is one of the most important 
areas underlying cloud computing and big-data processing, as well as 
our area of expertise.

------------

**TODO: update based on how article looks like**

\ 

We first describe the salient features of container technology that 
are relevant in the evaluation of experiments. We then describe what 
in our view is missing in order to make containers a useful 
reproducibility tool for storage systems research. We then present a 
use case that illustrates the container management extensions that we 
envision that would allow to record metadata about the execution of 
experiments. We then describe how this metadata can be used to 
evaluate an experiment and, at the same time, create a catalog that 
researches can use when looking for particular types of experiments. 
We then discuss the benefits and limitations of using containers as a 
way of reproducing research in other areas of experimental systems 
research. We conclude and delineate future work.

------------

# Experimental Evaluation of Computer Systems Research

An experiment in systems research is usually composed by a triplet of 
workload, a specific system where the workload runs and results from a 
particular execution. Respective to this order is the complexity 
associated to the evaluation of an experiment: obtaining the exact 
same results is more difficult than just getting access to the 
original workload. Thus, we can define a taxonomy to characterize the 
reproducibility of experiments:

 1. _Workload Reproducibility_. We have access to the original code 
    and the particular workload that was used to obtain the original 
    experimental results.
 2. _System Reproducibility_. We have access to hardware and software 
    resources that resemble the original dependencies.
 3. _Results Reproducibility_. The results of the re-execution of an 
    experiment are valid with respect to the original.

In storage systems research, workload reproducibility usually entails 
getting access to the configuration of the benchmarking tool that 
defines the IO patterns of the experiment. For example, if an 
experiment uses the Flexible IO Tester[^fio] (FIO), then the workload 
is defined by the FIO input file.

System reproducibility can be divided in software and hardware. The 
former corresponds to the entire software stack from the 
firmware/kernel up to the libraries used, whereas the latter comprises 
the same set of hardware devices involved in the experiment such as 
specific storage drives and network switches and cards.

Reproducing results does not necessarily imply the re-generation of 
the exact same performance metrics, instead it entails validating the 
original results by obtaining metrics that are very close (in shape or 
trends) to the original experiment. Given this, evaluating whether a 
re-run of an experiment reproduces the original results can be a 
subjective task. We propose specific definitions within the domain of 
storage systems in terms of resource utilization over time, for 
example, IO throughput (IOPS) of a storage device. **TODO:extend**

**TODO: talk about deployment, automation**. Container technology can 
be used to achieve 1 and partially 2, since a container keeps the 
snapshot of the entire system. The challenge is in the evaluation of 
results when the underlying OS kernel and hardware change, which we 
describe in the next section.

# Containers For Reproducible Research

For experiments suitable to be containerized ([see](#all) for a 
discussion), we define a mapping methodology that allows to evaluate 
experimental results on hardware distinct than the original:

  1. Obtain the profile of the original hardware
  2. Obtain the configuration of each container (e.g. User Beacons or 
     cgroup configuration)
  3. Obtain the profile of the new hardware
  4. Generate a configuration for the new hardware based on 2-4

We envision the following piece of infrastructure [^note]:

  0. Hardware profiling module that runs at multiple intervals (e.g. 
     daily, for every execution)
  1. A monitoring process on each host OS
  2. Notion of distributed execution of an experiment
  3. Monitor daemon keeps track of container metrics
  4. At the end of execution.

[^note]: **NOTE:** for the camera-ready version of this paper we will 
add a diagram of the architectural view of our extensions.

# Use Case: Reproducing Results on Different Storage Devices

We compare:

  * An old hard drive
  * The equivalent (modern) model of an old hard drive
  * An SSD

We will show that:

  * Direct I/O is reproducible
  * Asynchronous I/O is not
  * Therefore we need to extend containers to handle async requests on 
    a per-container basis (not on a global)

This case will also illustrate how having the HW profile and base 
configuration of containers can help in:

  * mapping to new hardware
  * mapping to old-but-aged hardware

# Discussion

We discuss other benefits and limitations of containerization, as well 
as general reproducibility guidelines when working with containers.

## Cataloging Experiments

We can characterize experiments by creating categories of container 
metric profiles, e.g.:

  * In-memory only
  * Storage intensive
  * Network intensive
  * CPU intensive
  * xx% of caching effects

We need to profile an experiment, store the profile along with the 
image of the container, categorize the profile and index it.

## Can All Systems Research Be Containerized? {#all}

Based on previous performance evaluations of container technology 
[@xavier_performance_2013 ; @felter_updated_2014 ; 
@xavier_performance_2014 ; @tang_performance_2014] we can extrapolate 
the following conditions for which experimental results will likely be 
affected by the implementation of the underlying OS-level 
virtualization:

  * Memory bandwidth is of significant importance (i.e. if 5% of 
    performance will affect results).
  * External storage drives can't be used, thus having the experiment 
    perform I/O operations within the filesystem namespace where the 
    container is located.
  * Network address translation (NAT) is required.
  * Distinct experiments consolidated on the same host.
  * For an experiment, containers with conflicting roles need to be 
    co-located in the same physical host (e.g. two database instances 
    on the same host).
  * Kernel version can't be frozen.

Any experiment for which any of the above applies should be carefully 
examined since the effects of containerization can affect the results. 
The design of the experiment should explicitly account for these 
effects.

## Other Lessons Learned So Far

We list some of the lessons that we have learned as part of our 
experience in implementing experiments in containers:

  * Version control everything and use git subtrees/submodules (or 
    alike) to keep track of inter-dependencies between projects. For 
    example, if a git repository contains the definition of a 
    container, make it a submodule of the main project.
  * Refer to the specific version ID that a paper's results were 
    obtained from. Git's tagging feature can also be used to point to 
    the version that contains the codebase for an experiment (e.g. 
    "sosp15").
  * When possible, add experimental results as part of the commit that 
    contains the codebase of an experiment. In other words, try to 
    make the experiment as self-contained as possible, so that 
    checking out that particular version contains all the dependencies 
    and generated data.
  * Keep a downloadable container image for the version of the 
    experiment codebase (e.g. use the docker registry and its 
    automated build feature).
  * Whenever possible, use CI technologies to ensure that changes to 
    the codebase don't disrupt the reproducibility of the experiment.
  * Obtain a profile of the hardware used (eg. making use of tools 
    such as SoSReport[^sosreport]), as well as the resource 
    configuration of every container and publish these as part of the 
    experimental results (i.e. add it to the commit that a paper's 
    results are based on).

# Related Work

The challenging task of reproducing experimental results in applied 
computer science research has been long recognized 
[@ignizio_establishment_1971 ; @ignizio_validating_1973 ; 
@crowder_reporting_1979]. This issue has has recently received a 
significant amount of attention from the computational research 
community [@freire_provenance_2008 ; @freire_computational_2012 ; 
@stodden_implementing_2014 ; @neylon_changing_2012 ; 
@cheney_provenance_2009 ; @leveqije_reproducible_2012], where the 
focus is more on numerical reproducibility rather than performance. 
Similarly, efforts such as _The Recomputation Manifesto_ 
[@gent_recomputation_2013] and the _Software Sustainability Institute_ 
[@crouch_software_2013] have reproducibility as a central part of 
their endeavour but leave runtime performance as a secondary problem. 
In systems research, runtime performance _is_ the subject of study, 
thus we need to look at it as a primary issue. By obtaining profiles 
of executions and making them part of the results, we allow 
researchers to validate experiments with performance in mind.

In [@collberg_measuring_2014], the authors took a long list of 
articles published in 13 top-tier conferences found that 25% of the 
articles are reproducible (under their reproducibility criteria). The 
authors did not analyzed performance. In our case, we are interested 
not only in being able to rebuild binaries and run them but also in 
evaluating the results.

Containers, and specifically docker, have been the subject of recent 
efforts that try to alleviate some of the reproducibility problems in 
data science [@boettiger_introduction_2014]. Existing tools such as 
Reprozip [@chirigati_reprozip_2013] package an experiment in a 
container without having to initially implement it in one (i.e. 
automates the creation of a container from an "non-containerized" 
environment). Our work is complementary in the sense that we look at 
the conditions in which the experiment can be validated in order tu 
ensure that the containerized version will (or won't) produce valid 
results.

# Conclusion and Future Work

Take papers from other sub-disciplines of Systems Research:

  * Kernel
  * Network
  * Hardware

How can we repeat the effort presented in this paper for the above?

# References

[^sosreport]: <https://www.github.com/sosreport/sos>

[^recomp]: <http://www.recomputation.org/manifesto>

[^fio]: <https://github.com/axboe/fio>

<!-- hanged biblio-->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
